%----------------------------------------------------------------------------------------
%	The Industry-Academia Knowledge Gap
%----------------------------------------------------------------------------------------
\chapter{Challenges Between Industry and Academia}\label{chap:gap}
\lhead{\emph{\nameref{chap:gap}}}

%\todo{Is there a better title for this?}

While the review in \Cref{chap:literature_review} indicates that \rea is a growing concern among \rt researchers, it's still only being addressed with any depth on a minority of secondary studies.
It is clear that several authors believe \rea is a challenge worth addressing in research, but there is not a lot of available \rt literature focusing on the steps that need to be taken in order to improve academia-industry communication and shorten the technology transfer gap.

We conclude this work by highlighting some key challenges that we have identified, combining data found in the literature itself, in the authors' responses and in the practitioner survey.
These are challenges that may have been addressed in certain circumstances but remain unsolved in a broad sense, as they are still present in several recent works.
Along with each challenge, we make some suggestions that could be applied by Software Engineering researchers and/or Software Testing practitioners --- these could be actionable steps for upcoming primary studies, or further avenues of investigation for secondary or meta studies.
\Cref{table:challenges} provides the summary of the challenges we identified, indicating the primary source of our observation (i.e. the literature, the authors and/or the practitioners).

\begin{table}[]
\centering
\scriptsize
\rowcolors{1}{}{gray!10}
%\setlength{\tabcolsep}{6pt}
\begin{tabular}{llllll|llllll}
\toprule
\textbf{ID} & \textbf{Title} & \textbf{L}  & \textbf{A} & \textbf{P} & \textbf{I} &
\textbf{ID} & \textbf{Title} & \textbf{L}  & \textbf{A} & \textbf{P} & \textbf{I}\\
\midrule
CH1 & Alignment of motivations             &   				&  			     & \fullcirc      & \fullcirc &
CH6 & Absence of TSR/TSA                   & \fullcirc 		&                & \fullcirc 	  & \fullcirc \\

CH2 & Realistic experimentation            & \fullcirc 		&                &                & &
CH7 & Clarity of target                    & \fullcirc 		&                &        	      & \\

CH3 & Scalability                          & \fullcirc 		&                &                & &
CH8 & Skepticism                           &                & 	 & 	  & \fullcirc \\

CH4 & Relevance of metrics                 & \fullcirc 		& \fullcirc 	 &                & &
CH9 & Data quality/availability        &                & 	 & 	  & \fullcirc \\

CH5 & Developing usable tools			   &            	& \fullcirc 	 & \fullcirc      & \fullcirc &
CH10 & Communication                       &                & \fullcirc 	 & \fullcirc 	  & \fullcirc \\
\bottomrule
\end{tabular}\\
\begin{flushleft}
\scriptsize Source(s): \textbf{L}: Literature; \textbf{A}: Author questionnaire; \textbf{P}: Practitioner survey; \textbf{I}: Industry partner.
\end{flushleft}
\caption{Summary of main challenges identified by this study.}
\label{table:challenges}
\end{table}

% ===================

\section{List of Challenges}
\label{sec:gap_challenges}

\subsection{CH1: Alignment of Motivations}
When asked what would convince them to implement and use an~\rt tool, eight surveyed practitioners gave responses that can be synthesized into ``\textit{it would make my work easier}''.
This general sentiment matches the responses received during interviews at our industrial partner.

There exists a mismatch between academic motivations and industrial needs: research is concerned with discovering novel techniques that might provide marginal effectiveness gains over the state-of-the-art, while practitioners are mostly concerned with any solution that simplifies their workflow.
In other words, even if an \rt technique has the potential to greatly reduce the testing time of a suite, practitioners will weigh those benefits against the effort required to implement the technique and adapt/maintain it for their needs.
This is not to say that the current research motivations are ill-informed: it is the role of academia to push the boundaries of what is possible in theory first, and sometimes this theory takes many years to find relevance in practice.
One interviewee from \Cref{chap:industry} gave an example of this: \quoter{mutation testing\footnote{Mutation testing refers to a set of techniques that introduce artificial errors in the \sut in order to validate the efficacy of a test suite \cite{jia2010analysis}.} has been in the literature since the 1990s, and it is starting to see adoption in industry today}{R8}.

If the researchers have the objective of implementing their approach, they must be certain that it is addressing the current needs of practitioners.
An obvious way to achieve this, which is also confirmed by our literature review, is by developing techniques through direct partnerships between academic researchers and industrial practitioners (or even open-source communities).
In these cases, the practitioners bring realistic examples of the challenges they face and, as a result, these collaborative works tend to produce results suitable for practical applications and could serve as a guideline for other, purely academic, approaches.

Naturally, not all research can be done with industrial partnerships, and in these cases there is difficulty in finding what exactly is relevant to current practitioners.
One possible source of this information is grey literature: information produced by experts in a field, but without necessarily following academic guidelines, in the form of blog posts, videos, magazine articles, talks etc.
Practitioners who produce grey literature can help inform researchers about the current state of practice, the main existing challenges in software development, and successful implementations of techniques (e.g. the aforementioned Netflix blog~\cite{netflixlerner} which details the process of bringing a technique from a paper into their workflow).

Ultimately, the unavoidable reality is that academics and practitioners work towards different goals.
Researchers are motivated and driven by publication; indeed, the point of research is to aggregate information into a body of knowledge that grows gradually over time.
Historically, academia does not encourage researchers to continue working on a project after the related paper is published and ``see it through'' to an eventual application of a technique.
In fact, the pursuit for novelty can diminish the perceived value of a researcher who is willing to perform experiments and do the additional work to implement a technique.

This is in direct contrast with the desires of industry.
Few companies are willing to commit time, money and human resources into developing novel techniques that may or may not provide value or savings in the long term.
Rather, they would rather adopt practices with a proven and predictable outcome.
%The space between novel and theoretically valid techniques and ones that ensure long-term value in practice is 

% ===================

\subsection{CH2: Realistic Experimentation}
It is clearly not possible for every research paper to feature practitioner co-authors or to rely on an industrial partnership for experimentations.
Selecting the right subject for experiments is a decisive point when writing a paper about a technique.
Older studies on \rt would often rely on the ``Siemens programs''~\cite{hutchins1994experiments}, which is believed to have caused an overfitting of results to a particular kind of software~\cite{do_recent_2016}.
More recently, the Software Infrastructure Repository (SIR)~\cite{do2005supporting} (e.g. \citepalias{schwartz_cost-effective_2016}) and Defects4J~\cite{just2014defects4j} (e.g. [\citetalias{noor_similarity-based_2016}, \citetalias{azizi_retest_2018}]) have been used to similar ends.
Having common subjects can provide replicability benefits when directly comparing techniques, although is not always clear if they approximate the difficulty of testing real software.
Authors who are able to collaborate directly with members of industry gain an enormous advantage if they are allowed to run experiments on production code, but it is also clear that not every paper will have that opportunity.

The most obvious alternative is to use large-scale open-source software (e.g. from the Mozilla \citepalias{zhou_beating_2020} and Apache 
[\citetalias{oqvist_extraction-based_2016}, 
\citetalias{bertolino_learning--rank_2020}, 
\citetalias{pan_dynamic_2020}, 
\citetalias{bagherzadeh_reinforcement_2022}, 
\citetalias{chen_context-aware_2021}]
 foundations) as subjects, since the communities developing these programs follow procedures much like the developers working for corporations .
This is also far from trivial.
The larger the software, the more time a researcher will need to dedicate in order to understand it and to adapt the technique to it, sacrificing the possibility of experimenting on a larger variety of subjects and thus again bringing the risk of overfitting.
Additionally, there is no established consensus regarding which properties an open-source program must satisfy in order to be a satisfactory subject.

Alleviating this issue would require effort from both researchers and practitioners.
For example, Google has an open dataset of testing results~\cite{googledataset}, and \citetalias{spieker_reinforcement_2017} combined it with one from ABB Robotics.
As a result, this combined dataset has already been used by other papers covering machine learning 
[\citetalias{wu_time_2019}, 
\citetalias{lima_multi-armed_2022}, 
\citetalias{pan_dynamic_2020}, 
\citetalias{sharif_deeporder_2021}, 
\citetalias{omri_learning_2022}].
Two practitioners mention that ``\textit{open source code/data is not provided}'' due to confidentiality reasons.
In those cases, our suggestion would be to provide some opaque information regarding the system, e.g. its programming language, the number of lines of code and/or tests, how many developers work on it, how frequently is the code updated, etc.
At the very least, this would help researchers choose subjects with similar characteristics.

% ===================

\subsection{CH3: Scalability}
\rt techniques provide the most savings when applied to large-scale software projects, which can have multiple thousands of test cases.
Therefore, it is important that techniques are designed to scale up to any size of test suite, but few papers tackle this issue directly.
The trouble is that scalability is very hard to measure unless multiple subjects of different sizes are used.
One way to demonstrate scalability, beyond relying on industrial partners or large-scale open-source projects, is to artificially generate large datasets (e.g.~[\citetalias{miranda_fast_2018}, \citetalias{cruciani_scalable_2019}]), which are useful from the algorithmic perspective, but might not address other issues that arise in large-scale software development.
It is also worth mentioning that many \rt techniques can become \textit{disadvantageous} when applied to small test suites, as the cost of running the technique does not outweigh the savings in testing time.
So selecting the size of the experiment subject is important both to highlight the scalability of the tool in large software and also to consider whether the necessary overhead is a deal-breaker on small or medium projects.


% ===================
\subsection{CH4: Relevance of Metrics}
\Cref{sec:lit_rq1} shows that a wide variety of metrics has been used to evaluate the effectiveness of \rt techniques.
Some are used almost universally for a certain kind of challenge (e.g. APFD for \tcp), while others have nearly no presence beyond the paper that introduced them.

The abundant use of APFD and its variants indicate that, at least among researchers, there is a consensus of its utility and importance when evaluating \tcp approaches, although the usage of specific variants might hamper that benefit.
At the same time, it is not clear that a technique optimized for only APFD is sufficient to satisfy the needs of software developers in practice.
Still, APFD has been in use for over 20 years and it cannot simply be dismissed: at the very least it provides an agreed-upon method of directly comparing different techniques.

For the cases of \tcs and \tsr, there is less controversy on what are the most important metrics; reduction rate and fault detection loss appear to be the consensus among researchers, and there are fewer novel and single-use metrics.
As an example,~\citetalias{mehta_data-driven_2021} interviewed practitioners at Microsoft before deciding on their \tcs metrics, obtaining three main targets: reduction of cost, reduction of time and the failure detection rate.
We can observe in~\Cref{sec:lit_rq1} that these concerns are reasonably addressed by \tcs techniques, although researchers still appear to prioritize reducing the selected set rather than ensuring all failures are detected.

The metrics of applicability and diagnosability \citepalias{correia_motsd_2019, zhou_beating_2020} are interesting propositions that consider other degrees of usefulness of a tool to developers.
Their existence indicates that some researchers still believe there is room for improved metrics that, perhaps, better map the requirements of real-world software, although these are rarely found in the literature.
Furthermore, ease-of-use is an important point to consider and, as far as we could detect, there is no established method of measuring it.

One practitioner stated: ``\textit{I don't think that academic tools are the best in a professional environment, I prefer commercial tools,}'' implying they believe academics are not measuring the results that matter most to them.
Indeed, managers allocating development funds will usually focus on the dollar savings a technique can bring, regardless of its theoretical effectiveness in fault-finding (as mentioned by respondent author \#43: \quotes{the cost associated with the 1\% bugs that were missed is too high}).

% ===================
\subsection{CH5: Converting Research into Usable Tools}
When techniques are designed in an academic context, they are normally developed as proof-of-concept works.
That is, the purpose is to show that the technique works and provides significant results according to some metrics.
However, this leads to two issues: either primary studies do not make their solution available for implementation, as we discussed in~\Cref{sec:lit_rq2}, or their experiments do not thoroughly consider practical concerns such as efficiency or the data requirements of a proposed approach.
Finally, what seems to matter the most is time and budget for developing a tool.
Papers are usually written targeting a hard deadline and their prototypes often do not see further work past publication.
It is inevitable that researchers will move on to new challenges, but their contribution would be amplified if the tool is, at the very least, open-source and well-documented so that other interested parties can continue the work in the future if desired.

If an \rt technique is implemented as a prototype that is shown to work on a certain kind of software, it is much easier to get the attention from a practitioner and convert the solution into something used in practice.
If feasible, an available prototype with solid documentation and usage instructions can be valuable both for study replicability and as a way to get developers interested in using it.
That said, the responsibility of developing fully functional tools should not fall solely upon researchers.
One practitioner stated that ``\textit{[\rt tools] need full security screening}'', and other said ``\textit{it requires an adaptation}''; these steps are not actionable by researchers in isolation.
As industry stands to benefit from scientific advances, it should be in its best interest to promote and fund the collaborations needed to continue development of promising prototypes.

This strongly relates to \textbf{CH1} regarding the motivations of academics and practitioners.
Even if a researcher has the desire to see a technique through to its applicability, or even just to provide a robust source code for the implementation of an approach, there is little incentive from the academic side for doing so.
Certain software engineering conferences have started to encourage or even require the inclusion of replication packages and source code of studies with an empirical component, which is a step in the right direction.

%\todo{Whose responsibility is it? A PhD student for example could work on this, but are there academic/publishing motivations for it?}

% ===================
\subsection{CH6: Absence of \tsr/\tsa}
Out of \numpapers papers, only 8 are about \tsr and, surprisingly, only one covers \tsa 
 \citepalias{yoshida_fsx_2016}.
60\% of the surveyed practitioners claim that ``creating or updating tests'' is a major challenge in real-world \rt, so the desire for \tsa exists and there appears to be ample room for experimenting with new approaches and metrics.
However, most practitioners interviewed at the industry partner claim that increasing test scope is not a major concern, as even the manually-written tests can be too many.

On the other hand, they do mention the difficulty of refactoring and removing obsolete test cases as a frequent challenge, which aligns with 47\% of the surveyed practitioners.
\tsr could prove valuable to testers who need to manage ever-growing test suites and hardly find the time to manually assess tests that are obsolete or in need of refactoring.
In reality, this is often addressed by simply increasing the capacity of testing servers, but this is a costly and non-scalable solution.

This assessment of \tsr and \tsa techniques indicate an opportunity for researchers to develop novel methods for these challenges and to progress in directions that are in need of improvement in software development workflows.

% ===================
\subsection{CH7: Clarity of Target}
Several of the papers we reviewed don't clearly state key characteristics of their SUT, such as its programming language or its scale (either in lines of code or test cases).
For practitioners and other researchers to consider a paper worthy of investigation, it is important to know for which kind of system a piece of research was designed.

As mentioned in~\Cref{sec:lit_rq2}, few \rt techniques are language-agnostic and many do not inform the target language at all.
Similarly, the type of software (web, mobile, embedded, distributed, etc.) or its development paradigm are important factors to mention, seen in studies such as~\citetalias{zhong_testsage:_2019} for web services and~\citetalias{lima_multi-armed_2022} for software developed and delivered through continuous integration.
Not every tool can be used in any type of software, and it is likely that specific types of software might require specific solutions, so it is important to state the particularities of certain subject programs.
This is akin to the point of ``context factors'' brought up by~\citet{bin_ali_search_2019}, which helps to alleviate the issue by introducing a base taxonomy that can be used to categorize techniques.

Critically, there is often ambiguity on the very definition of test case.
Software testing can include unit tests, integration tests, multi-component tests, system tests, end-to-end tests and so forth.
Most papers do not make it explicit which layer of testing it is addressing. While it can sometimes be inferred with some domain knowledge, it is difficult to be certain for most readers.
This information would be valuable for interested practitioners and also for researchers who are looking to identify gaps in the literature.
On top of that, some papers use the term ``test case'' to refer to test methods, while others use it when referring to test classes/files (which contain several test methods), so the granularity of the technique is not always clear, and this can impact both effectiveness and efficiency analysis.
This challenge can be solved by having a paragraph dedicated to explicitly describing the properties and context factors of the experiment subjects.

% ===================
\subsection{CH8: Skepticism}
In general, there is some degree of skepticism from practitioners regarding automated solutions.
For example, some interviewees at the industrial partner expressed concerns that a \tcs solution might leave out an important test, or that \tcp algorithms might always prioritize the same tests.
Their intuition is that performing selection (or even prioritization) manually allows for easier troubleshooting in case something goes wrong.
As for what can go wrong, the fear is to detect a fault later on, only to find out the test to detect it already existed, but was not selected or prioritized by the automated tool.

This calls back to a comment made by one of the responding authors in \Cref{sec:lit_rq2}: even if 99\% of faults are detected at a fraction of the testing costs, the remaining 1\% of slips is unnerving to the people responsible for the tests.
And even if a tool is shown to be safe, there is still a feeling that there \textit{might} be a slip that will only be revealed much later than it should have been.

Regarding \tsr, most developers are also opposed to outright removing tests, but are willing to put these tests in less-frequent rotations, e.g. weekly instead of daily.
Finally, for \tsa the practitioners are also doubtful that automatically generated or enhanced tests are as good as human-designed ones, but wouldn't be against to trying it if the need arises.

% ===================
\subsection{CH9: Data Quality and Availability}
Another point raised by the interviewed practitioners at the industrial partner is their own ability to provide the data required by an automated tool.
History-based approaches, for example, are more effective when there are many test logs archived for analysis, but in reality these logs are only stored temporarily.

Furthermore, even if the data exists it might not be of sufficient quality to, for example, serve as a training set for a machine learning model.
This extends to the test code itself, as highlighted by one of the interviewees:
\quoter{researchers might assume a certain level of quality or standardization among test cases, but in reality test design is often bad}{R8}.
If the existing test cases are not standardized and high-quality, automated attempts to improve them are unlikely to yield the desired benefits.

% ===================
\subsection{CH10: Communication}
The main challenge, which connects most of the previous ones, is communication.
Researchers and practitioners both lead busy lives, focusing on their day-to-day affairs, and ultimately communication between the two realms suffers.

There are some steps that can be taken to improve this.
Companies can start by having round-table discussions on recent research publications (e.g. the Google Journal Club \cite{googlejournal}) and, if possible, they should invite the author(s) to participate.
On the other side, universities can host lectures by practitioners in addition to other researchers.
This can start small --- find people in the same city, perhaps alumni of the university, who are working on something interesting and have a conversation.

Out of the practitioners we surveyed for \Cref{chap:literature_review}, 56\% claimed they keep contact with a friend or colleague who is a researcher in Software Engineering.
After all, most academics have interacted with people who are currently practitioners during their education process, and vice-versa.
This means that both sides have an opportunity to network and communicate beyond their current professions, giving each other ideas of what is currently relevant in industrial software development and what is the latest state-of-the-art in academic research.
Unfortunately, the practitioners interviewed for \Cref{chap:industry} have mostly lost contact with their academic colleagues and today struggle to keep up with the advancement of research.

It can be a daunting idea to catch up to latest research trends, so larger companies could consider having employees dedicated to understanding the internal processes and challenges while searching for collaborations with academics, or at least promote internal reading club sessions.
Many researchers would be thrilled to receive a message inviting them for a joint effort with palpable outcomes.

\section{Threats to Validity}
\label{sec:gap_threats}

The list of challenges assembled in \Cref{chap:gap} is based on our own observations of the current state of industry-academia relations from a multitude of sources: the literature, communications with other authors, feedback from practitioners and our own experiences developing techniques.
It is not meant to be a comprehensive and end-all checklist of challenges to solve, as other researchers following different sources would likely come to a divergent set of conclusions.
That said, we believe that most researchers performing a similar study would agree upon the majority of the listed challenges.
To the extent of our knowledge, these challenges are real and in need of further study, but there is no guarantee that addressing each one of them will solve all the problems with software engineering research.