%----------------------------------------------------------------------------------------
%	Conclusion
%----------------------------------------------------------------------------------------
\chapter{Conclusion}\label{chap:conclusion}
\lhead{\emph{Conclusion}} % Set the left side page header to "Introduction"

\section{Threats to Validity}\label{sec:threats}

\subsection{Threats to \Cref{chap:literature_review} }

\paragraph{Construct validity} Despite our efforts to comprehensively find all primary studies that meet our selection criteria, we might have missed some.
To mitigate this threat, we performed a systematic search over five broad digital libraries and complemented the search with a snowballing cycle and a check with authors of all found studies, who in fact suggested a few additional entries.

As usual for this kind of study, our selection of papers was performed through queries, followed by manual filtering.
To diminish potential bias of the latter step, the filtering process was systematically reviewed and agreed upon among all the three authors.

\paragraph{Internal validity} The internal validity of this study is strongly dependent on the three research questions that guided all our analysis as well as the data extraction form we  built.
We took great care in ensuring that they properly reflect our objectives, although it is unavoidable that, by formulating  different questions or using other data extraction forms, we could have obtained other results.
We might also have overlooked or misinterpreted some important information or arguments in the primary studies, beyond our best efforts and accuracy in the full reading of all selected papers.
To mitigate such threats we provide all extracted data in traceable format, highlighting the main points we extracted from each primary study.
Furthermore, the responses we received directly from authors often provide additional context that reduce the risk of misinterpretation.
That said, we cannot make the full responses available due to non-disclosure requests from some authors.

\paragraph{Conclusion validity}
The conclusions we drew in terms of  the information  we summarize from the primary studies, the detected challenges we discuss in the above section and the recommendations we formulate in the conclusions might have been influenced by our background, and other authors might have reached different conclusions.
Such potential bias is unavoidable in this type of study, however we tried to mitigate it by aiming at full consensus of all authors behind each conclusion.
Furthermore, by documenting in detail the data extraction process, we ensure a fully transparent study that can be verified and replicated.
The survey sent to practitioners helps to validate our conclusions.
Although the sample of 23 responses is very small, it shows a degree of alignment among people working in six different countries.
A convenience sample was used to distribute the survey; thus, the practitioners we reached are more likely to have some contact with ongoing research.
To avoid excessive bias in that direction, we did not contact members of industry who are known to regularly publish in Software Engineering events.

\paragraph{External validity}
We do not make any claim of validity of our conclusions beyond the \numpapers papers analyzed.
As more primary studies are published, they should be read and analyzed on their own, and our conclusions should be revised accordingly. In consideration of this threat, in the aim of ensuring validity even in future, we are committed to keep the live repository up-to-date, taking into account the community inputs.
Moreover, we believe that the framework we developed consisting of the three research questions, the data extraction form and the structured tables for summarizing the approaches and the metrics could be still applicable also by other external authors.

\subsection{Threats to \Cref{chap:orchestration} }

We evaluated \fz using faults available in \dfj.  Our results and
conclusions could be different had we used another bug repository.
However, \dfj is among the most popular bug repositories and is
heavily used in research on regression testing.  Additionally, it
includes real faults, which strengthens our findings.

The fact that we use \dfj means that we were running experiments on
project versions that are potentially very far apart (e.g., years).
In this setup, \ek might select a very large number of tests, because
it was designed for small code changes between two
consecutive commits~\cite{gligoricEk, VasicETAL17EkstaziSharp}.  However, 
\ek ended up performing well
even in our setup.

We defined the testing budget as the number of tests that one can run
at each project version, which does not take into account the
differences in individual test execution time.  As we focus on unit
tests, we do not expect that there would be substantial differences in
execution time across tests.

To measure effectiveness, we used \ttff and \apfd. As known the \dfj subjects contain only one fault per version and hence the two measures behave similarly. 
To mitigate this issue, we need to perform more studies on subjects containing multiple faults, for which the \apfd measure becomes more valuable. 

In our experiments we assume that test execution is deterministic,
which we know  does not always hold in practice, i.e.,
tests are flaky~\cite{luo2014empirical,harman2018start}.  We have not observed any flaky behavior in our
experiments: only the expected set of tests was failing in each run.

\subsection{Threats to \Cref{chap:industry} }

These observations relate to a few teams at Ericsson and might not generalize to software industry as a whole.

\subsection{Threats to \Cref{chap:gap} }