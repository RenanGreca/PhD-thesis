\section{Test Suite Orchestration}\label{sec:orchestration}

Given the challenges associated with ever-expanding regression testing suites of continuously evolving software, we 
\textit{Test suite orchestration} is the art of generating, choosing, prioritizing and executing tests in order to maximize the effectiveness of testing while keeping costs within a desired budget.
Today, research on test orchestration is quite granular, with individual researchers mostly focusing on specific challenges within this topic.
While this is important for the continuity and advancement of research, it fails in addressing the practical concerns of software developers, who desire a complete solution to aid the development cycle.

Features such as \textit{test case generation}, \textit{test case prioritization}, handling of \textit{flaky tests}, \textit{mutation testing}, \textit{test suite augmentation} and others can be considered under the broader scope of test orchestration.
While improvements in each of these features can provide substantial benefits, it is their combination that can produce the desired solution.

In general, test suite orchestration can be thought of as a broad challenge with the ultimate goal of improving regression testing in multiple aspects, composed of several sub-challenges.
These sub-challenges include, but are not limited, to the following:
\begin{itemize}
	\item \textbf{Test case selection (\tcs):} the challenge of determining a sub-set of tests that, when executed, provides sufficiently high confidence that recent changes have not introduced failures in the software, while substantially reducing the execution costs.
	\item \textbf{Test case prioritization (\tcp):} the challenge of ordering tests to detect potential faults as early as possible, prioritizing tests that are most likely to reveal faults or that cover critical parts of the program.
	\item \textbf{Test suite reduction or minimization (\tsr):} the challenge of reducing the test suite by finding and possibly removing redundant tests. Unlike \tcs, which is change-aware, \tsr.
	\item \textbf{Test suite amplification or augmentation:} the challenge of expanding and improving an existing test suite through various different means. A survey on test suite amplification is found in \cite{danglot_snowballing_2019}; out of the categories presented, the synthesis of new tests with respect to changes is the most relevant for a continuously-evolving system. 
	\item \textbf{Handling of unreliable/flaky tests:} a test that might pass or fail non-deterministically without changes to the SUT is designated as unreliable or flaky. This can happen due to poor test design, misconfiguration of the test suite or the testing environment, or timing errors in asynchronous tasks. These tests make it difficult for developers to identify true faults in the system and thus they should ideally be detected and flagged as such.
	\item \textbf{Test uncertainty:} the challenge of considering uncertain factors in software development, such as human input, values generated by machine learning, or cyber-physical interactions. In \cite{garlan_software_2010}, the sources, implications and challenges of uncertainty in software engineering are explored.
	\item \textbf{Compositional testing:} the challenge of guaranteeing correctness of a whole system by individually testing its distinct components. For example, a system using multi-component testing should be able to rely on the preceding single-component tests being correct. This challenge is mentioned in \cite{harman_start-ups_2018}, where the authors suggest the need of mock functions and analysis that can ``begin anywhere''.
	\item \textbf{Incremental testing:} the challenge of testing new parts of a software without necessarily having to re-test the whole software. In \cite{harman_start-ups_2018} and \cite{ohearn_continuous_2018}, the notion of using procedure summaries as a way of handling incrementality is mentioned. This way, it is possible to use previous executions of the test suite to accelerate its execution in the future when only small parts of code are added or changed.
\end{itemize}

Individually, each of these challenges can be its own field of research, and indeed many works have been published on them.
However, an ideal test orchestration solution should consider all or most of these challenges in unison, as solving each one alone is not sufficient to solve the problems faced by software developers in practice.

Due to the breadth of the orchestration challenge, for this thesis the decision was made to restrict the scope and focus primarily on four aspects: test case prioritization, test case selection, test suite reduction/minimization and test suite amplification/augmentation.
Other topics remain tangential to the research and may occasionally be part of the discussion, but are not the focus of this work.
The following subsection offers definitions for the four groups of techniques that this study focuses on.
In \autoref{sec:lit_rq1}, common approaches and metrics for each group are described in more detail, according to data extracted from the existing literature.

\subsection{Test Case Prioritization}
\label{sec:tcp}

One challenge of regression testing is to detect failing tests fast.
The objective of \textit{test case prioritization} (\tcp) is to re-order test cases according to some definition of priority, in order to get faster feedback from the test execution.
Given an SUT $M$ and its test suite $T$,
\tcp can be described as a function $P(M, T)$ that provides a permutation of $T$, $T'$ such that, given a metric function $f$, $f(T') > f(T)$.
The optimal prioritization is one where $f(T')$ is greater than or equal to any other possible permutation of $T$. 

Some criteria often used for \tcp include: \textit{(1)} similarity-based, which attempts to diversify the execution of tests; \textit{(2)} coverage-based, with the objective of maximizing block\footnote{A \textit{block} of the SUT can be one line of code, a branch, a function, etc. according to system design and other necessities.} coverage with as few tests as possible; or \textit{(3)} history-based, which prioritizes tests that have a history of failing or revealing faults~\cite{khatibsyarbini_test_2018}.

Common metrics include: \textit{(1)} average percentage of faults detected (APFD), which estimates how effective a prioritization is in detecting faults in fewer tests; \textit{(1)} tests till first fault (TTFF), a count of how many tests were executed until one failed; or \textit{(3)} developer feedback time, a measure of how long it takes for a developer to get a report if there is a failing test in the suite.

\tcp is particularly useful in situations where the test suite is exceptionally large and detecting failures sooner allows for potential faults to be addressed quicker.
It's also relevant in cases where the testing budget is limited but not consistent, so testing might stop at any time and only tests that failed until then can be added to a report.

A prioritized test suite still contains all test cases, 
so there is no loss of failures detection ability (assuming that test results are independent and the testing budget is sufficient) -- what changes is the amount of time that it takes for one or more failures to be detected.


\subsection{Test Case Selection}
\label{sec:tcs}

In regression testing, not all tests are relevant to a particular code change:
if only a small part of one file was updated, it is unlikely that the entire project would be affected and the full regression test suite would have to be run.
\textit{Test case selection} (\tcs) addresses the challenge of selecting a subset of tests that is representative of the entire suite in a given situation~\cite{YooHarman10RegressionTestingSurvey, RothermelHarrold94FrameworkForEvaluationRTS}.
In other words, given subsequent versions of an SUT, $M$ and $M'$ and its test suite $T$, 
\tcs can be described as a function $S(M, M', T)$ that selects a subset $T' \subseteq T$ to be used for testing $M'$, considering the differences between $M$ and $M'$.

We say that a \tcs technique is \emph{safe} if it guarantees that all tests whose outcome may be affected by a change are included in the selected subset~\cite{RothermelHarrold94FrameworkForEvaluationRTS}.
That is, safe selection techniques output a subset $T'$ while maintaining the output of a fault detection metric function $f(T') \geq f(T)$.

Examples of approaches for \tcs are: \textit{(1)} change-based, which executes tests that have some relation to modified files, classes, or methods; \textit{(2)} model-based, which uses data extracted from models of the SUT to determine test execution; or \textit{(3)} graph-based, which uses a graph representation of the SUT to detect control flow and select relevant tests~\cite{kazmi_effective_2017}.

Some metrics for \tcs are: \textit{(1)} selection count or percentage, which measures how many tests were executed in comparison to the original suite (e.g. $|T'| \leq |T|$); \textit{(2)} testing time, or the time taken to execute the selected subset of tests; and \textit{(3)} fault detection capability, used to determine the safeness of the proposed technique.

A potential drawback of \tcs is that, depending on the size of the test suite and the execution time of individual tests, it may happen that the time needed to produce the subset $T'$ is greater than the savings provided by executing $T'$ instead of $T$. 

\subsection{Test Case Reduction and Minimization}
\label{sec:tsr}

Without considering subsequent versions of the SUT, \textit{test suite reduction} (\tsr) aims to find a minimal subset of test cases such that the testing requirements are still met.
Thus, given an SUT $M$ its test suite $T$ that satisfies a set of requirements $\{r_1, ..., r_n\}$, we describe \tsr as a function $R(M, T, r)$ which outputs a test suite $T' \subseteq T$ such that each $r_i$ is satisfied.

There exists conceptual overlap between \tcs and \tsr, with the key differences being change-awareness and the objective of the result.
While \tcs uses a comparison between versions of the SUT and produces a set of tests meant to validate those changes, \tsr can be performed on an isolated iteration of a program and is meant to detect tests that are no longer needed for full satisfaction of the requirements.
\tsr approaches are often coverage-based or requirements-based and, as discussed in \autoref{sec:lit_rq1}, evaluation metrics for \tcp and \tsr are often shared, since both are concerned with running fewer tests and reducing the overall testing time; however \tsr must ensure that there is no loss in fault detection capability in the long-term evolution of the suite.

Regarding the terms reduction and minimization, both options are used nearly interchangeably in the literature.
According to \citeauthor{yoo2012regression}, the difference in terminology is subtle: while both remove tests from the suite, ``minimization'' implies this change is temporary, while ``reduction'' stands for permanent removal of tests.
Generally speaking, the same techniques can be applied for both ends so, from the perspective of researchers, the two terms are not distinct.

\subsection{Test Case Amplification and Augmentation}\label{sec:tsa}
