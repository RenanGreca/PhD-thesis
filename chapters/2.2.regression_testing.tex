\section{Regression Testing of Evolving Software Systems}
\label{sec:regression}

In the early stages of commercial software development, computer programs were designed, produced and distributed mostly like physical retail goods.
That is, there was an initial planning and design phase, followed by an extensive development period and, on a certain deadline, the software was shipped embedded with hardware, or pressed onto floppy disks or CDs to be made available in shelves.
The advent of the Internet made it possible to completely alter this paradigm.
Now, these three phases still exist in commercial software, but happen much faster and can be repeated iteratively as needed.
In other words, software companies can initially design and develop the ``minimum viable product'' to be delivered to customers online and, with the software already in use, updates can be develop to add new features, improve existing ones, or correct bugs that can be detected\footnote{This is not the same for all types of software; embedded systems cannot always rely on the ability of pushing patches; and video games, for example, generally deliver complete products on a given deadline to account for distribution and marketing schedules, but even they almost always have an extensive post-release update cycle.}.
We denote software developed and released in this fashion as \textit{continuously evolving software}.
It can also happen that programs that were originally designed according to the traditional release cycle are, at some point, adapted and converted to be continuously evolving (e.g. Microsoft Windows shifted from yearly ``service packs'' to weekly online updates).

The shift to evolving software, which correlates to the pivot to agile development practices in the mid-2000s, also caused a significant change to how software testing is viewed and addressed.
Previously, it was common to have team members solely responsible for testing the developers' code, often times manually.
Nowadays, it is common practice for developers to write and test their own code, and have an active role in the maintenance of the regression testing suite.

\textit{Regression testing} is the part of software testing concerned with testing previously existing components of a system to guarantee that recent changes in the codebase did not affect the originally specified functionality of components.
This process is often one of the costliest aspects of software development \cite{rothermel_improving_2018}, as it should ideally be performed every time a code change is committed, and involves much repetition of previously performed tests.
It is defined in \cite{minhas_regression_2017} as ``an activity which makes sure that everything is working correctly after changes to the system.''
That is, its primary objective is to assure that, after each change to the software, previously existing code continues to comply to specification (or simply to expectations, in case no formal specification exists).

The term \textit{regression testing} (\rt) has its origins in pre-agile days and, as a research topic, has been studied since the 1980s~\cite{leung1989insights,yoo2012regression}.
At the time, release schedules were centered around a hard deadline, so \rt was an activity that was only performed near the end of the cycle, after the important features of the release had already been developed.
At that point, testers would check if any of the new changes interfered with previous functionality of the software; in some places this was a manual process, in others semi-automated.
Doing so earlier was not advantageous --- if a bug is detected in the middle of development but new features are not yet complete, it is possible that another bug will be detected on another round of testing.
Since the software could only be shipped once all features were done, intermediary regression testing provided little benefits.

Continuously evolving software shifted this dynamic.
With smaller and more frequent release cycles, regression testing too became a more frequent activity.
At the same time, the incredible feature speed demanded by customers and consumers means that it is not viable to postpone testing until right before release --- if a bug is detected at that point, it might be too late to fix it before delivery.
Thus, with the development of Continuous Integration/Continuous Delivery (CI/CD) practices and tools, automated regression testing became commonplace, sometimes executed as frequently as new code changes were pushed into a repository.

Test automation mostly solves the problem in small projects, where it takes only a few seconds or maybe minutes to run a full test suite.
Large-scale software demand additional attention because of two factors: the test suite is large and takes a long time to execute, and code commits arrive at such a high frequency that there isn't enough time to run the test suite between each commit.
Often, a combination of both factors become a major challenge in large-scale software development \cite{memon_taming_2017}.

In order to maintain the health of the testing process and the availability of testing equipment, the execution time of a suite should be less than the average time between commits pushed by developers.
In reality, this is difficult to achieve and maintain, as test suites tend to increase in scale (according to the necessities of an ever-evolving software) and commit frequency remains stable or can even decrease if new developers are added to the team.
The straightforward solution is to increase the computational power of testing servers, so testing time reduces by brute force, although obviously this incurs additional costs.

The concept of software size and scale is fundamental for the motivation of this research.
There are multiple ways of measuring software scale --- it could refer to a large number of lines of code (LOC); it can also mean high-complexity algorithms that run for a long time, or software that needs to serve multiple users simultaneously.
For this research, we are considered primarily with the number of test cases that the program needs to be reliable.
Thus, other aspects of software scalability go beyond the scope of this thesis and, here, the term ``scalability'' itself refers primarily to the ability of managing an ever-growing number of test cases.

That said, in this work, we are interested in ``industrial-scale evolving software''.
Understand ``industrial-scale'' as a general term for large-scale software in the real world.
In practice, it can mean several different kinds of software, such as software developed as the primary product of a corporation (in the technology industry), software that provides essential features to other products (such as in the automotive or telecommunication industries), or open-source software that is developed by a community instead of a team within a company.

It is also noteworthy that software can exist in a multitude of contexts --- e.g. embedded software, distributed systems, web or mobile applications, cloud-based solutions, and so forth.
Each context is associated with unique challenges that inevitably alter how they are designed.
For the most part, this thesis explores testing strategies that can be generalized into most contexts, as long as the software is continuously evolving in nature, although the ultimate implementation of these strategies might require adjustments.
