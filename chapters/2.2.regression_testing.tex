\section{Regression Testing of Evolving Software Systems}
\label{sec:regression}

In the early stages of commercial software development, computer programs were designed, produced and distributed mostly like physical retail goods.
That is, there was an initial planning and design phase, followed by an extensive development period and, on a certain deadline, the software would be shipped embedded with hardware, or pressed onto disquettes or CDs that could be mailed to customers or made available in store shelves.
The advent of the Internet made it possible to completely alter this paradigm.
Now, these three phases still exist in commercial software, but happen much faster and can be repeated iteratively as needed.
In other words, software companies can initially design and develop the ``minimum viable product'' to be delivered to customers online and, with the software already in use, updates can be develop to add new features, improve existing ones, or correct bugs that can be detected\footnote{This is not the same for all types of software; for example, embedded systems cannot always rely on the ability of online updates; meanwhile video games generally deliver complete products on a given deadline to account for distribution and marketing schedules, often followed by an extensive post-release update cycle which resembles the evolving software paradigm.}.
 
We denote software developed and released as ever-evolving products as \textit{continuously evolving software}.
The concept of evolving software was introduced in the 1970s by \citet{Lehman1980}, although it was in the 1990s that the term and paradigm gained widespread use, due to the accelerated delivery methods becoming available \cite{Mens2008}.
It can also happen that programs that were originally designed according to the traditional release cycle are, at some point, adapted and converted to be continuously evolving (e.g. Microsoft Windows shifted from yearly ``service packs'' to weekly online updates).

The shift to evolving software, which correlates to the pivot to agile development practices in the mid-2000s, also caused a significant change to how software testing is viewed and addressed.
Previously, it was common to see testing as its own stage of development; certain teams had members solely responsible for testing the source code, which was usually a manual process.
Nowadays, it is common practice for developers to write and test their own code, and have an active role in the maintenance of the regression testing suite, a practice encouraged by the agile method \cite{planview_agile_testing}.
This has the advantage of speeding up the testing process, although as a drawback it can cause testing to be seen as a ``second-class citizen'' by developers, who would rather create new features than test existing ones.

\textit{Regression testing} is the part of software testing concerned with testing previously existing components of a system to guarantee that recent changes in the codebase did not affect the originally specified functionality of components.
This process is one of the costliest aspects of software development \cite{rothermel_improving_2018}, as it should ideally be performed every time a code change is committed, and involves much repetition of previously performed tests.
It is defined in \cite{minhas_regression_2017} as ``an activity which makes sure that everything is working correctly after changes to the system.''
That is, its primary objective is to assure that, after each change to the software, previously existing code continues to comply to specification (or simply to expectations, in case no formal specification exists).

The term \textit{regression testing} (\rt) has its origins in pre-agile days and, as a research topic, has been studied since the 1980s~\cite{leung1989insights,yoo2012regression}.
At the time, release schedules were centered around a hard deadline, so \rt was an activity that was only performed near the end of the cycle, after the important features of the release had already been developed.
At that point, testers would check if any of the new changes interfered with previous functionality of the software; in some places this was a manual process, in others semi-automated.
Doing so earlier was not advantageous --- if a bug is detected in the middle of development but new features are not yet complete, it is possible that another bug will be detected on another round of testing.
Since the software could only be shipped once all features were done, intermediary regression testing provided little benefits.

Continuously evolving software shifted this dynamic.
With smaller and more frequent release cycles, regression testing too became a more frequent activity.
At the same time, the incredible feature speed demanded by customers and consumers means that it is not viable to postpone testing until right before release --- if a bug is detected at that point, it might be too late to fix it before delivery.
Thus, with the development of Continuous Integration/Continuous Delivery (CI/CD) practices and tools, automated regression testing became commonplace, sometimes executed as frequently as new code changes are pushed into a repository.

Test automation mostly solves the problem in small projects, where it takes only a few seconds or maybe minutes to run a full test suite.
Large-scale software demand additional attention because of two factors: the test suite is large and takes a long time to execute, and code commits arrive at such a high frequency that there is not enough time to run the test suite between each commit.
Often, a combination of both factors become a major challenge in large-scale software development \cite{memon_taming_2017}.

In order to maintain the health of the testing process and the availability of testing equipment, the execution time of a suite should be less than the average time between commits pushed by developers.
In reality, this is difficult to achieve and maintain, as test suites tend to increase in scale (according to the necessities of an ever-growing software) and commit frequency remains stable or can even increase if new developers are added to the team.
The straightforward solution is to increase the computational power of testing servers, so testing time reduces by brute force, although obviously this incurs additional costs.

The concept of software size and scale is fundamental for the motivation of this research.
There are multiple ways of measuring software scale --- it could refer to a large number of lines of code (LOC); it can also mean high-complexity algorithms that run for a long time, or software that needs to serve multiple users simultaneously.
For this research, we are considered primarily with the number of test cases that the program needs to be reliable.
Thus, other aspects of software scalability go beyond the scope of this thesis and, here, the term ``scalability'' itself refers primarily to the ability of managing an ever-growing number of test cases.

That said, in this work, we are interested in ``industrial-scale evolving software''.
Understand ``industrial-scale'' as a general term for large-scale software in the real world.
In practice, it can mean several different kinds of software, such as software developed as the primary product of a corporation (in the technology industry), software that provides essential features to other products (such as in the automotive or telecommunication industries), or open-source software that is developed by a community instead of a team within a company.

It is also noteworthy that software can exist in a multitude of contexts --- e.g. embedded software, distributed systems, web or mobile applications, cloud-based solutions, and so forth.
Each context is associated with unique challenges that inevitably alter how they are designed.
For the most part, this thesis explores testing strategies that can be generalized into most contexts, as long as the software is continuously evolving in nature, although the ultimate implementation of these strategies might require adjustments.
