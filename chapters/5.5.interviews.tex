\section{Interviews}
\label{sec:ind_interviews}


Information about how testing is performed at Ericsson and regarding the challenges still faced at the company was gathered via a series of interviews.
This was initially in the form of unstructured conversation, while the interviewer understood the central details.
After the basics were covered, we performed a series of 30- to 60-minute sessions with 1 to 3 people at a time asking more focused questions.

\autoref{table:interviewees} lists the roles of the interviewees, who are anonymized for this study.

\begin{table}[]
\centering
%\scriptsize
\rowcolors{1}{}{gray!10}
%\setlength{\tabcolsep}{6pt}
\begin{tabular}{ll}
\toprule
\textbf{ID} & \textbf{Role} \\
\midrule
R1 & Functional Area Domain Tester \\
R2 & Functional Area Domain Tester \\
R3 & Functional Area Domain Tester \\
R4 & Continuous Integration Test Manager \\
R5 & Module Test Manager \\
R6 & Module Test Manager \\
R7 & Senior Test Specialist \\
R8 & Senior Test Specialist \\
\bottomrule
\end{tabular}\\
\caption{Interviewees at Ericsson.}
\label{table:interviewees}
\end{table}

\subsection{Current practices}

Regarding the current testing practices at Ericsson, we want to understand what are the day-to-day activities performed by the team.
We would also like to know if there are any implementations of the regression testing techniques classically studied in research (selection, prioritization, reduction, amplification).

The interviewed team members are mostly working on multi-component testing (MCT) and have all mentioned two key acronyms for this layer: \quoter{SDC for delivery, SBT for nightly runs}{R1}.
SDC, or Source Delivery Check, is a short execution of the test suite that happens whenever a developer pushes changes to be merged into the main branch of a module.
Due to time and resource constraints, \quoter{for SDC there is selection, in nightly we run everything}{R1}.
This is also called the ``gating loop'', since a failing test prevents the change from being merged.
At the time of the interviews, \quoter{it's manually decided what goes in SDC}{R6} and \quoter{the running time [...] for MCT is 15-20 mins}{R5}.
SBT, or Source Baseline Test, is the nightly run of the test suite, also known as ``assessment loop''.
Here, all tests in the suite are executed, which can take up to 10 hours.
As a general rule, the tests in SDC are a subset of the SBT.

\paragraph{Selection.} In MCT, it is manually determined whether a test should be included in the SDC: \quoter{It's manually decided what goes in SDC}{R6}; \quoter{yesterday, we got the question, `should we include this in gating?' I don't know, I just go by feeling. We have to see if the feature seems fundamental or important to Ericsson somehow}{R5}.
Despite this practice, most of the interviewees are aware that there could be a better way of doing things: \quoter{I think there should be more strategy than just me thinking}{R5}.
When this finding was reported to R8, who oversees testing all over Ericsson and does not work with R1-7 on a daily basis, they mentioned that the company does have an internal tool for test selection named RENSA and was surprised to find out this team did not use it.
R4 explains: \quotes{we had attempts to integrate RENSA but it did not work out so well. I think we never built a business case for 5G and it ended up way in the backlog. In 2017 or 2018 we said `this is something we should add to our plan', but we did not do anything for 5G.} 
However, \quoter{it might be deployed in other parts of Ericsson}{R7}.
That said, R8 also expressed some skepticism towards \tcs: \quoter{similarity-based selection misses boundary values. Why do you need to select? The agile principle says we should test everything}{R8}.

\paragraph{Prioritization.} This does not appear to be an active concern for the team and there are no techniques in place for advanced prioritization: \quoter{for prioritization, nothing specific}{R1}.
\quoter{We have suites, which are groups, e.g. for sub-modules, gating or not. Then I think it's just the order they're written.}{R5}.
However, \quoter{now we have introduced shuffling for the assessment}{R5}, i.e. random ordering of the test cases.
The motivation for this is not decreasing the feedback time, but rather \quoter{to help find problems with unstable tests}{R1}, which might be flaky according to the execution order.
R8 again expressed a concern regarding \tcp: \quoter{using the same TCP approach every time, wouldn't the same test be top priority every time?}{R8}.

\paragraph{Reduction and Amplification.} These techniques are absent in the workflow. \quoter{We don't have any tools that helps us in any way in shrinking or expanding the tests}{R5}. \quoter{No, we don't have anything like that}{R6}.
Regarding reduction, \quoter{if it happens that there are too many tests for overnight, there would be an initiative to reduce}{R1}, but \quoter{we can put more machines and we can run more}{R6}.
Interviewees generally agree that it is more cost-efficient to increase computing capacity of testing servers than to spend human time determining which tests to remove.
For amplification, there appears to be no interest in automating the process.
There is a protocol, however, to manually augment the test suite in the situations where a fault slips through a layer of testing: \quoter{if a fault was found later in test or in the field, we try to investigate why did it slip through, and we should have a test for that}{R1}; then, \quoter{after the fact, if there is time, we can go back and understand why a needed test was not in the plan}{R2}.


\subsection{Roles and experience}

R1, R2 and R3 are members of the Functional Area Domain team.
They describe themselves as \quoter{the owners of the test suite}{R2} and are responsible for \quoter{monitoring the test results}{R2}.
In particular, they \quoter{are monitoring the nightly runs}{R1}, i.e. the SBT or non-blocking tests.

R4, R5 and R6 are test managers, albeit R4 has a different responsibility.
The module test managers are responsible for the testing of particular modules at Ericsson, meaning \quoter{not working on specific features}{R5}.
\quoter{I also guide the teams on how to write test cases [...], how we organize test suites and so on}{R6}.
Meanwhile, the CI Test Manager is \quoter{responsible for the machines and environment and some of the test framework parts. To simplify, we are doing the framework, the developers are writing the tests, the managers are designing the strategy}{R4}.

Finally, R7 and R8 are designated as test specialists, meaning they handle longer-term strategy.
\quoter{I work a lot with test strategies. How we should test features and products [...] from now until 2025}{R7}.
Despite the similar titles, there is a key difference between the two specialists: R7 is responsible for a long-term strategy of a specific core of Ericsson products, while R8 has an overview of the entire company, so their job also includes sharing technologies and strategies among distant teams.

Aside from R8, who has a PhD on the topic of software testing, most of the interviewees did not have testing as a focus of their education while at university.
When asked about their education, R4, R5 and R6 are computer scientists and/or engineers, and most of the testing knowledge they had prior to working at Ericsson was \quoter{standard university [curriculum], which contains tests}{R5}.
Meanwhile, R7 \quotes{graduated in media and communication}, so testing was \quotes{not covered in university}.
R4, R5 and R7 mention having a certification by the ISQTB\footnote{International Software Testing Qualifications Board}.
Additionally, R4 also mentions taking testing courses led by R8 internally at Ericsson.

\subsection{Pain points}

\subsection{Collaboration with Academia}
