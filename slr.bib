
@article{ramler_tool_2017,
	title = {Tool support for change-based regression testing: {An} industry experience report},
	volume = {269},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010211135&doi=10.1007%2f978-3-319-49421-0_10&partnerID=40&md5=99503cb5610a14936b640833f23c16a5},
	doi = {10.1007/978-3-319-49421-0_10},
	abstract = {Changes may cause unexpected side effects and inconsistencies. Regression testing is the process of re-testing a software system after changes have been made to ensure that the new version of the system has retained the capabilities of the old version and that no new defects have been introduced. Regression testing is an essential activity, but it is also time-consuming and costly. Thus, regression testing should concentrate on those parts of the system that have been modified or which are affected by changes. Regression test selection has been proposed over three decades ago and, since then, it has been frequently in the focus of empirical studies. However, regression test selection is still not widely adopted in practice. Together with the test team of an industrial software company we have developed a tool-based approach that assists software testers in selecting regression test cases based on change information and test coverage data. This paper describes the main usage scenario of the approach, illustrates the implemented solution, and reports on its evaluation in a large industry project. The evaluation showed that the tool support reduces the time required for compiling regression test suites and fosters an accurate selection of regression test cases. The paper concludes with our lessons learned from implementing the tool support in a real-world setting. © Springer International Publishing AG 2017.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Ramler, R. and Salomon, C. and Buchgeher, G. and Lusser, M.},
	year = {2017},
	keywords = {\_tablet},
	pages = {133--152},
	file = {2017 Tool support for change-based regression testing.pdf:/Users/renangreca/Zotero/storage/HGBG9CWR/2017 Tool support for change-based regression testing.pdf:application/pdf},
}

@article{ouriques_test_2018,
	title = {Test case prioritization techniques for model-based testing: a replicated study},
	volume = {26},
	issn = {15731367},
	doi = {10.1007/s11219-017-9398-y},
	abstract = {Recently, several test case prioritization (TCP) techniques have been proposed to order test cases for achieving a goal during test execution, particularly, revealing faults sooner. In the model-based testing (MBT) context, such techniques are usually based on heuristics related to structural elements of the model and derived test cases. In this sense, techniques' performance may vary due to a number of factors. While empirical studies comparing the performance of TCP techniques have already been presented in literature, there is still little knowledge, particularly in the MBT context, about which factors may influence the outcomes suggested by a TCP technique. In a previous family of empirical studies focusing on labeled transition systems, we identified that the model layout, i.e., amount of branches, joins, and loops in the model, alone may have little influence on the effectiveness of TCP techniques investigated, whereas characteristics of test cases that actually fail definitely influences this aspect. However, we considered only synthetic artifacts in the study, which reduced the ability of representing properly the reality. In this paper, we present a replication of one of these studies, now with a larger and more representative selection of techniques and considering test suites from industrial systems as experimental objects. Our objective is to find out whether the results remain while increasing the validity in comparison to the original study. Results reinforce that there is no best performer among the investigated techniques and characteristics of test cases that fail represent an important factor, although adaptive random-based techniques are less affected by it.},
	number = {4},
	journal = {Software Quality Journal},
	author = {Ouriques, João Felipe S. and Cartaxo, Emanuela G. and Machado, Patrícia D.L.},
	month = dec,
	year = {2018},
	note = {Publisher: Springer New York LLC},
	keywords = {\_tablet, Empirical evaluation, Fault detection, Model-based testing, Test case prioritization},
	pages = {1451--1482},
	file = {2018 Test case prioritization techniques for model-based testing.pdf:/Users/renangreca/Zotero/storage/9NPNY9KA/2018 Test case prioritization techniques for model-based testing.pdf:application/pdf},
}

@article{gotlieb_using_2017,
	title = {Using global constraints to automate regression testing},
	volume = {38},
	url = {https://doi.org/10.1609/aimag.v38i1.2714},
	abstract = {Communicating or autonomous systems rely on high-quality software-based components. that must be thoroughly verified before they are released and deployed in operational settings. Regression testing is a crucial verification process that compares any new release of a software-based component against its previous versions, by executing available test cases. However, limited testing time makes selection of test cases in regression testing challenging, and some selection criteria must be respected. Validation engineers usually address this problem, coined as test suite reduction (TSR), through manual analysis or by using approximation techniques. In this paper, we address the TSR problem with sound artificial intelligence techniques such as constraint programming (CP) and global constraints. By using distinct cost-value-aggregating criteria, we propose several constraint-optimization models to find a subset of test cases that cover all the test requirements and optimize the overall cost of selected test cases. Our contribution includes reuse of existing preprocessing rules to simplify the problem before solving it and the design of structure-aware heuristics that take into account the notion of the costs associated with test cases. The work presented in this paper has been motivated by an industrial application in the communication domain. Our overall goal is to develop a constraint-based approach of test suite reduction that can be deployed to test a complete product line of conferencing systems in continuous delivery mode. By implementing this approach in a software prototype tool and experimentally evaluating it on both randomly generated and industrial instances, we hope to foster a quick adoption of the technology. Copyright © 2017, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	number = {1},
	journal = {AI Magazine},
	author = {Gotlieb, A. and Marijan, D.},
	year = {2017},
	keywords = {\_tablet},
	pages = {73--87},
	file = {2017 Using global constraints to automate regression testing.pdf:/Users/renangreca/Zotero/storage/BITF7DMG/2017 Using global constraints to automate regression testing.pdf:application/pdf},
}

@article{tahvili_cost-benefit_2016,
	title = {Cost-benefit analysis of using dependency knowledge at integration testing},
	volume = {10027 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998880972&doi=10.1007%2f978-3-319-49094-6_17&partnerID=40&md5=78c0b820adfb65385dc736b5fd3653e5},
	doi = {10.1007/978-3-319-49094-6_17},
	abstract = {In software system development, testing can take considerable time and resources, and there are numerous examples in the literature of how to improve the testing process. In particular, methods for selection and prioritization of test cases can play a critical role in efficient use of testing resources. This paper focuses on the problem of selection and ordering of integration-level test cases. Integration testing is performed to evaluate the correctness of several units in composition. Further, for reasons of both effectiveness and safety, many embedded systems are still tested manually. To this end, we propose a process, supported by an online decision support system, for ordering and selection of test cases based on the test result of previously executed test cases. To analyze the economic efficiency of such a system, a customized return on investment (ROI) metric tailored for system integration testing is introduced. Using data collected from the development process of a large-scale safety-critical embedded system, we perform Monte Carlo simulations to evaluate the expected ROI of three variants of the proposed new process. The results show that our proposed decision support system is beneficial in terms of ROI at system integration testing and thus qualifies as an important element in improving the integration testing process. © Springer International Publishing AG 2016.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Tahvili, S. and Bohlin, M. and Saadatmand, M. and Larsson, S. and Afzal, W. and Sundmark, D.},
	year = {2016},
	keywords = {\_tablet},
	pages = {268--284},
	file = {2016 Cost-benefit analysis of using dependency knowledge at integration testing.pdf:/Users/renangreca/Zotero/storage/WAA3ZLG4/2016 Cost-benefit analysis of using dependency knowledge at integration testing.pdf:application/pdf},
}

@article{blondeau_test_2017,
	title = {Test case selection in industry: an analysis of issues related to static approaches},
	volume = {25},
	issn = {15731367},
	doi = {10.1007/s11219-016-9328-4},
	abstract = {Automatic testing constitutes an important part of everyday development practice. Worldline, a major IT company, is creating more and more tests to ensure the good behavior of its applications and gains in efficiency and quality. But running all these tests may take hours. This is especially true for large systems involving, for example, the deployment of a web server or communication with a database. For this reason, tests are not launched as often as they should be and are mostly run at night. The company wishes to improve its development and testing process by giving to developers rapid feedback after a change. An interesting solution is to reduce the number of tests to run by identifying only those exercising the piece of code changed. Two main approaches are proposed in the literature: static and dynamic. The static approach creates a model of the source code and explores it to find links between changed methods and tests. The dynamic approach records invocations of methods during the execution of test scenarios. Before deploying a test case selection solution, Worldline created a partnership with us to investigate the situation in its projects and to evaluate these approaches on three industrial, closed source, cases to understand the strengths and weaknesses of each solution. We propose a classification of problems that may arise when trying to identify the tests that cover a method. We give concrete examples of these problems and list some possible solutions. We also evaluate other issues such as the impact on the results of the frequency of modification of methods or considering groups of methods instead of single ones. We found that solutions must be combined to obtain better results, and problems have different impacts on projects. Considering commits instead of individual methods tends to worsen the results, perhaps due to their large size.},
	number = {4},
	journal = {Software Quality Journal},
	author = {Blondeau, Vincent and Etien, Anne and Anquetil, Nicolas and Cresson, Sylvain and Croisy, Pascal and Ducasse, Stéphane},
	month = dec,
	year = {2017},
	note = {Publisher: Springer New York LLC},
	keywords = {\_tablet, Dynamic, Industrial case, Static, Test selection},
	pages = {1203--1237},
	file = {2017 Test case selection in industry.pdf:/Users/renangreca/Zotero/storage/Z5Z25ACN/2017 Test case selection in industry.pdf:application/pdf},
}

@inproceedings{busjaeger_learning_2016,
	title = {Learning for test prioritization: {An} industrial case study},
	volume = {13-18-November-2016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997235107&doi=10.1145%2f2950290.2983954&partnerID=40&md5=615068ccb51cecdca7962c441cefab77},
	doi = {10.1145/2950290.2983954},
	abstract = {Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration envi-ronments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimiza-tion mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in indus-trial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques.},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Busjaeger, B. and Xie, T.},
	year = {2016},
	keywords = {\_tablet, Regression testing, Test prioritization, Learning to rank},
	pages = {975--980},
	file = {2016 Learning for test prioritization.pdf:/Users/renangreca/Zotero/storage/6734KRHH/2016 Learning for test prioritization.pdf:application/pdf},
}

@inproceedings{bach_coverage-based_2017,
	title = {Coverage-{Based} {Reduction} of {Test} {Execution} {Time}: {Lessons} from a {Very} {Large} {Industrial} {Project}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018454606&doi=10.1109%2fICSTW.2017.6&partnerID=40&md5=7edc272a800065f17d9103a19e60de94},
	doi = {10.1109/ICSTW.2017.6},
	abstract = {There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50\% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97\%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects. © 2017 IEEE.},
	booktitle = {Proceedings - 10th {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops}, {ICSTW} 2017},
	author = {Bach, T. and Andrzejak, A. and Pannemans, R.},
	year = {2017},
	keywords = {\_tablet, Test case prioritization, Coverage, Large real world project, Test case selection, Test specificty},
	pages = {3--12},
	file = {2017 Coverage-Based Reduction of Test Execution Time.pdf:/Users/renangreca/Zotero/storage/JNLI4ZLN/2017 Coverage-Based Reduction of Test Execution Time.pdf:application/pdf},
}

@inproceedings{wu_time_2019,
	address = {New York, NY, USA},
	series = {Internetware '19},
	title = {A {Time} {Window} {Based} {Reinforcement} {Learning} {Reward} for {Test} {Case} {Prioritization} in {Continuous} {Integration}},
	isbn = {978-1-4503-7701-0},
	url = {https://biblioproxy.cnr.it:2481/10.1145/3361242.3361258},
	doi = {10.1145/3361242.3361258},
	abstract = {Continuous integration refers to the practice of merging the working copies of all developers into the mainline frequently. Regression testing for each mergence is characterized by continually changing test suite, limited execution time, and fast feedback, which demands new test optimization techniques. Reinforcement learning is introduced for test case prioritization to save computing resources in continuous integration environment, where a reasonable reward function is highly important for learning strategy, since the process of reinforcement learning is a reward-guided behavior. In this paper, APHFW, a novel reward function is proposed by using partial historical information of test cases effectively for fast feedback and cost reduction. The experiments are based on three open-source data sets, and the results show that the proposed reward function is more cost-effect than other reinforcement learning rewards in continuous integration environment.},
	booktitle = {Proceedings of the 11th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Zhaolin and Yang, Yang Y. and Li, Zheng and Zhao, Ruilian},
	month = oct,
	year = {2019},
	keywords = {\_tablet, Test case prioritization, continuous integration, Continuous integration, reinforcement learning, Reinforcement learning, reward function, Reward function, test case prioritization},
	file = {2019 A Time Window Based Reinforcement Learning Reward for Test Case Prioritization.pdf:/Users/renangreca/Zotero/storage/8VCZEZRG/2019 A Time Window Based Reinforcement Learning Reward for Test Case Prioritization.pdf:application/pdf},
}

@article{noemmer_evaluation_2020,
	title = {An {Evaluation} of {Test} {Suite} {Minimization} {Techniques}},
	volume = {371 LNBIP},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078426536&doi=10.1007%2f978-3-030-35510-4_4&partnerID=40&md5=7e66adb7ed268d857ce23a02cccbe96c},
	doi = {10.1007/978-3-030-35510-4_4},
	abstract = {As a software project evolves over time, the associated test suite usually grows with it. If test suites are not carefully maintained, this can easily result in massive test execution duration, reducing the benefits of regression testing because faults are found later in development or even after release. Test suite minimization aims to combat long running test suites by removing redundant test cases. Previous work mainly evaluates test suite minimization techniques based on comparably small projects, which are less practically relevant. In this paper, we compare four test suite minimization techniques by applying them to several open source software projects and evaluate the results. We find that the size and execution time of all the test suites can be reduced by over 70\% on average. However, there is a substantial loss in fault detection capability of, on average, around 12.5\%, restricting the applicability of this form of test suite minimization. © Springer Nature Switzerland AG 2020.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Noemmer, R. and Haas, R.},
	year = {2020},
	keywords = {\_tablet},
	pages = {51--66},
	file = {2020 An Evaluation of Test Suite Minimization Techniques.pdf:/Users/renangreca/Zotero/storage/A3CUMR9V/2020 An Evaluation of Test Suite Minimization Techniques.pdf:application/pdf},
}

@article{srikanth_test_2016,
	title = {Test case prioritization of build acceptance tests for an enterprise cloud application: {An} industrial case study},
	volume = {119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975883639&doi=10.1016%2fj.jss.2016.06.017&partnerID=40&md5=2fd0b23046071b1b4a25dbab3b359637},
	doi = {10.1016/j.jss.2016.06.017},
	abstract = {The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't. © 2016 Elsevier Inc.},
	journal = {Journal of Systems and Software},
	author = {Srikanth, H. and Cashman, M. and Cohen, M.B.},
	year = {2016},
	keywords = {\_tablet, Regression testing, Prioritization, Cloud computing, Software as a service},
	pages = {122--135},
	file = {2016 Test case prioritization of build acceptance tests for an enterprise cloud.pdf:/Users/renangreca/Zotero/storage/YPWVUGD3/2016 Test case prioritization of build acceptance tests for an enterprise cloud.pdf:application/pdf},
}

@inproceedings{leong_assessing_2019,
	address = {Montreal, QC, Canada},
	title = {Assessing {Transition}-{Based} {Test} {Selection} {Algorithms} at {Google}},
	isbn = {978-1-72811-760-7},
	url = {https://ieeexplore.ieee.org/document/8804429/},
	doi = {10.1109/ICSE-SEIP.2019.00019},
	abstract = {Continuous Integration traditionally relies on testing every code commit with all impacted tests. This practice requires considerable computational resources, which at Google scale, results in delayed test results and high operational costs. To deal with this issue and provide fast feedback, test selection and prioritization methods aim to execute the tests which are most likely to reveal changes in test results as soon as possible. In this paper we present a simulation framework to support the study and evaluation, with real data, of such techniques. We propose a test selection algorithm evaluation method, and detail several practical requirements which are often ignored by related work, such as the detection of transitions, the collection and analysis of data, and the handling of ﬂaky tests. Based on this framework, we design an experiment evaluating ﬁve potential regression test selection algorithms, based on simple heuristics and inspired by previous research, though the evaluation technique is applicable to any number of algorithms for future experiments. Our results show that algorithms based on the recent (transition) execution history do not perform as well as expected (given the previously reported results) and that the test selection problem remains largely open. We found that the best performing algorithms are based on the number of times a test has been triggered and the number of distinct authors committing code that triggers particular tests. More research is needed in order to close the gap between the current approaches and the optimal solution.},
	language = {en},
	urldate = {2019-11-07},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Leong, Claire and Singh, Abhayendra and Papadakis, Mike and Le Traon, Yves and Micco, John},
	month = may,
	year = {2019},
	keywords = {\_tablet, regression testing, industry, continuous integration, Regression Testing, Continuous Integration, google},
	pages = {101--110},
	file = {2019 Assessing Transition-Based Test Selection Algorithms at Google.pdf:/Users/renangreca/Zotero/storage/59QNGLSL/2019 Assessing Transition-Based Test Selection Algorithms at Google.pdf:application/pdf},
}

@inproceedings{correia_motsd_2019,
	title = {{MOTSD}: {A} multi-objective test selection tool using test suite diagnosability},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071946861&doi=10.1145%2f3338906.3341187&partnerID=40&md5=36b1cf919148903948b4d80095863d54},
	doi = {10.1145/3338906.3341187},
	abstract = {Performing regression testing on large software systems becomes unfeasible as it takes too long to run all the test cases every time a change is made. The main motivation of this work was to provide a faster and earlier feedback loop to the developers at OutSystems when a change is made. The developed tool, MOTSD, implements a multi-objective test selection approach in a C\# code base using a test suite diagnosability metric and historical metrics as objectives and it is powered by a particle swarm optimization algorithm. We present implementation challenges, current experimental results and limitations of the tool when applied in an industrial context. Screencast demo link: {\textless}a{\textgreater}https://www.youtube.com/watch?v=CYMfQTUu2BE{\textless}/a{\textgreater} © 2019 ACM.},
	booktitle = {{ESEC}/{FSE} 2019 - {Proceedings} of the 2019 27th {ACM} {Joint} {Meeting} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Correia, D. and Abreu, R. and Santos, P. and Nadkarni, J.},
	year = {2019},
	keywords = {\_tablet, Test selection, Diagnosability, Feedback, Multi-objective},
	pages = {1070--1074},
	file = {2019 MOTSD.pdf:/Users/renangreca/Zotero/storage/NER7DFSQ/2019 MOTSD.pdf:application/pdf},
}

@inproceedings{noor_similarity-based_2016,
	title = {A similarity-based approach for test case prioritization using historical failure data},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964844271&doi=10.1109%2fISSRE.2015.7381799&partnerID=40&md5=8b4ae21b489d5486900b7c408b2b890f},
	doi = {10.1109/ISSRE.2015.7381799},
	abstract = {Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures. © 2015 IEEE.},
	booktitle = {2015 {IEEE} 26th {International} {Symposium} on {Software} {Reliability} {Engineering}, {ISSRE} 2015},
	author = {Noor, T.B. and Hemmati, H.},
	year = {2016},
	keywords = {\_tablet},
	pages = {58--68},
	file = {2016 A similarity-based approach for test case prioritization using historical.pdf:/Users/renangreca/Zotero/storage/2NB54D98/2016 A similarity-based approach for test case prioritization using historical.pdf:application/pdf},
}

@inproceedings{aman_application_2016,
	title = {Application of {Mahalanobis}-{Taguchi} {Method} and 0-1 {Programming} {Method} to {Cost}-{Effective} {Regression} {Testing}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018371174&doi=10.1109%2fSEAA.2016.29&partnerID=40&md5=78971d58e6fdf2e809af86b7463f301a},
	doi = {10.1109/SEAA.2016.29},
	abstract = {To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods. © 2016 IEEE.},
	booktitle = {Proceedings - 42nd {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications}, {SEAA} 2016},
	author = {Aman, H. and Tanaka, Y. and Nakano, T. and Ogasawara, H. and Kawahara, M.},
	year = {2016},
	keywords = {\_tablet, Regression testing, -1 programming method, Mahalanobis-Taguchi method},
	pages = {240--244},
	file = {2016 Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to.pdf:/Users/renangreca/Zotero/storage/T2W3AYJV/2016 Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to.pdf:application/pdf},
}

@inproceedings{najafi_improving_2019,
	title = {Improving {Test} {Effectiveness} {Using} {Test} {Executions} {History}: {An} {Industrial} {Experience} {Report}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072121509&doi=10.1109%2fICSE-SEIP.2019.00031&partnerID=40&md5=d291b1db592197696c8ac0eee02e836a},
	doi = {10.1109/ICSE-SEIP.2019.00031},
	abstract = {The cost of software testing has become a burden for software companies in the era of rapid release and continuous integration. Our industrial collaborator Ericsson also faces the challenges of expensive testing processes which are typically part of a complex and specialized testing environment. In order to assist Ericsson with improving the test effectiveness of one of its large subsystems, we adopt test selection and prioritization approaches based on test execution history from prior research. By adopting and simulating those approaches on six months of testing data from our subject system, we confirm the existence of valuable information in the test execution history. In particular, the association between test failures provide the most value to the test selection and prioritization processes. More importantly, during this exercise, we encountered various challenges that are unseen or undiscussed in prior research. We document the challenges, our solutions and the lessons learned as an experience report. Our experiences can be valuable for other software testing practitioners and researchers who would like to adopt existing test effectiveness improvement approaches into their work environment. © 2019 IEEE.},
	booktitle = {Proceedings - 2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}, {ICSE}-{SEIP} 2019},
	author = {Najafi, A. and Shang, W. and Rigby, P.C.},
	year = {2019},
	keywords = {\_tablet, Test prioritization, Test selection, Industrial experience report, Test effectiveness},
	pages = {213--222},
	file = {2019 Improving Test Effectiveness Using Test Executions History.pdf:/Users/renangreca/Zotero/storage/ZNZHSL6W/2019 Improving Test Effectiveness Using Test Executions History.pdf:application/pdf},
}

@inproceedings{zhong_testsage:_2019,
	address = {Xi'an, China},
	title = {{TestSage}: {Regression} {Test} {Selection} for {Large}-{Scale} {Web} {Service} {Testing}},
	isbn = {978-1-72811-736-2},
	shorttitle = {{TestSage}},
	url = {https://ieeexplore.ieee.org/document/8730207/},
	doi = {10.1109/ICST.2019.00052},
	abstract = {Regression testing is an important but expensive activity in software development. Among various types of tests, web service tests are usually one of the most expensive (due to network communications) but widely adopted types of tests in commercial software development. Regression test selection (RTS) aims to reduce the number of tests which need to be retested by only running tests that are affected by code changes. Although a large number of RTS techniques have been proposed in the past few decades, these techniques have not been adopted on large-scale web service testing. This is because most existing RTS techniques either require direct code dependency between tests and code under test or cannot be applied on large scale systems with enough efﬁciency. In this paper, we present a novel RTS technique, TestSage, that performs RTS for web service tests on large scale commercial software. With a small overhead, TestSage is able to collect ﬁne grained (function level) dependency between test and service under test that do not directly depend on each other. TestSage has also been successfully applied to large complex systems with over a million functions. We conducted experiments of TestSage on a large scale backend service at Google. Experimental results show that TestSage reduces 34\% of testing time when running all AEC (Analysis, Execution and Collection) phases, 50\% of testing time while running without collection phase. TestSage has been integrated with internal testing framework at Google and runs day-to-day at the company.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	publisher = {IEEE},
	author = {Zhong, Hua and Zhang, Lingming and Khurshid, Sarfraz},
	month = apr,
	year = {2019},
	keywords = {\_tablet, industry, google},
	pages = {430--440},
	file = {2019 TestSage.pdf:/Users/renangreca/Zotero/storage/RVTWI935/2019 TestSage.pdf:application/pdf},
}

@inproceedings{shi_evaluating_2018,
	address = {Amsterdam Netherlands},
	title = {Evaluating test-suite reduction in real software evolution},
	isbn = {978-1-4503-5699-2},
	url = {https://dl.acm.org/doi/10.1145/3213846.3213875},
	doi = {10.1145/3213846.3213875},
	abstract = {Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.},
	language = {en},
	urldate = {2021-11-22},
	booktitle = {Proceedings of the 27th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Shi, August and Gyori, Alex and Mahmood, Suleman and Zhao, Peiyuan and Marinov, Darko},
	month = jul,
	year = {2018},
	keywords = {\_tablet},
	pages = {84--94},
	file = {2018 Evaluating test-suite reduction in real software evolution.pdf:/Users/renangreca/Zotero/storage/ZZY6TH9L/2018 Evaluating test-suite reduction in real software evolution.pdf:application/pdf},
}

@article{chi_multi-level_2017,
	title = {Multi-{Level} {Random} {Walk} for {Software} {Test} {Suite} {Reduction}},
	volume = {12},
	issn = {1556-603X},
	url = {http://ieeexplore.ieee.org/document/7895279/},
	doi = {10.1109/MCI.2017.2670460},
	abstract = {Which test cases should be selected to save the time of software testing? Due to the large time cost of running all test cases, it is necessary to run representative test cases to shorten the software development cycle. Test suite reduction, an NP-hard problem in software engineering, aims to select a subset of test cases to reduce the time cost of test execution in satisfying test requirements. Recently, search based software engineering provides a new direction to test suite reduction by connecting software engineering problems with computational intelligence methods. In this paper, we propose a multi-level optimization algorithm to simplify the original problem instance of test suite reduction. In each level, we search for local optimal solutions with random walk in potential subsets of the test suite. The problem scale is reduced by locking the intersection of local optima and by discarding shielded test cases with no con­­ tribution to test requirements. We compare our algorithm with state-of-the-art methods on test suites of ten large-scale open source projects. Experiments show that our algorithm can more efficiently find optima on five out of six projects, in which Integer Linear Programming (ILP) can find optima; for the other four projects that ILP fails to solve, our algorithm provides the best solutions among heuristics in comparison.},
	language = {en},
	number = {2},
	urldate = {2021-11-22},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Chi, Zongzheng and Xuan, Jifeng and Ren, Zhilei and Xie, Xiaoyuan and Guo, He},
	month = may,
	year = {2017},
	keywords = {\_tablet},
	pages = {24--33},
	file = {2017 Multi-Level Random Walk for Software Test Suite Reduction.pdf:/Users/renangreca/Zotero/storage/2FSS5L6S/2017 Multi-Level Random Walk for Software Test Suite Reduction.pdf:application/pdf},
}

@inproceedings{lima_multi-armed_2020,
	address = {Natal Brazil},
	title = {Multi-{Armed} {Bandit} {Test} {Case} {Prioritization} in {Continuous} {Integration} {Environments}: {A} {Trade}-off {Analysis}},
	isbn = {978-1-4503-8755-2},
	shorttitle = {Multi-{Armed} {Bandit} {Test} {Case} {Prioritization} in {Continuous} {Integration} {Environments}},
	url = {https://dl.acm.org/doi/10.1145/3425174.3425210},
	doi = {10.1145/3425174.3425210},
	abstract = {Continuous Integration (CI) practices lead the software to be integrated and tested many times a day, usually subject to a test budget. To deal with this scenario, cost-effective test case prioritization techniques are required. COLEMAN is a Multi-Armed Bandit approach that learns from the test case failure-history the best prioritization order to maximize early fault detection. Reported results show that COLEMAN has reached promising results with different test budgets and spends, in the worst case, less than one second to execute. However, COLEMAN has not been evaluated against a search-based approach. Such an approach can generate near-optimal solutions but is not suitable to the CI budget because it takes too long to execute. Considering this fact, this paper analyses the trade-offs of the COLEMAN solutions in comparison with the near-optimal solutions generated by a Genetic Algorithm (GA). We use measures, which better fit with time constraints: Normalized Average Percentage of Faults Detected (NAPFD), Root-Mean-Square-Error (RMSE), and Prioritization Time. We use seven large-scale real-world software systems, and three different test budgets, 10\%, 50\%, and 80\% of the total time required to execute the test set available for a CI cycle. COLEMAN obtains solutions near to the GA solutions in 90\% of the cases, but scenarios with high volatility of test cases and a small number of cycles hamper the prioritization.},
	language = {en},
	urldate = {2021-11-22},
	booktitle = {Proceedings of the 5th {Brazilian} {Symposium} on {Systematic} and {Automated} {Software} {Testing}},
	publisher = {ACM},
	author = {Lima, Jackson A. Prado and Vergilio, Silvia R.},
	month = oct,
	year = {2020},
	keywords = {\_tablet},
	pages = {21--30},
	file = {2020 Multi-Armed Bandit Test Case Prioritization in Continuous Integration.pdf:/Users/renangreca/Zotero/storage/B59VTY3D/2020 Multi-Armed Bandit Test Case Prioritization in Continuous Integration.pdf:application/pdf},
}

@article{hirzel_graph-walk-based_2016,
	title = {Graph-{Walk}-based {Selective} {Regression} {Testing} of {Web} {Applications} {Created} with {Google} {Web} {Toolkit}},
	abstract = {Modern web applications are usually based on JavaScript. Due to its loosely typed, dynamic nature, test execution is time expensive and costly. Techniques for regression testing and fault-localization as well as frameworks like the Google Web Toolkit (GWT) ease the development and testing process, but still require approaches to reduce the testing eﬀort. In this paper, we investigate the eﬃciency of a specialized, graph-walk based selective regression testing technique that aims to detect code changes on the client side in order to determine a reduced set of web tests. To do this, we analyze web applications created with GWT on diﬀerent precision levels and with varying lookaheads. We examine how these parameters aﬀect the localization of client-side code changes, run time, memory consumption and the number of web tests selected for re-execution. In addition, we propose a dynamic heuristics which targets an analysis that is as exact as possible while reducing memory consumption. The results are partially applicable on non-GWT applications. In the context of web applications,we see that the eﬃciency relies to a great degree on both the structure of the application and the code modiﬁcations, which is why we propose further measures tailored to the results of our approach.},
	language = {en},
	author = {Hirzel, Matthias and Klaeren, Herbert},
	year = {2016},
	keywords = {\_tablet},
	pages = {15},
	file = {2016 Graph-Walk-based Selective Regression Testing of Web Applications Created with.pdf:/Users/renangreca/Zotero/storage/AWWYW3PA/2016 Graph-Walk-based Selective Regression Testing of Web Applications Created with.pdf:application/pdf},
}

@inproceedings{shi_understanding_2019,
	address = {Berlin, Germany},
	title = {Understanding and {Improving} {Regression} {Test} {Selection} in {Continuous} {Integration}},
	isbn = {978-1-72814-982-0},
	url = {https://ieeexplore.ieee.org/document/8987498/},
	doi = {10.1109/ISSRE.2019.00031},
	abstract = {Developers rely on regression testing in their continuous integration (CI) environment to ﬁnd changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Shi, August and Zhao, Peiyuan and Marinov, Darko},
	month = oct,
	year = {2019},
	keywords = {\_tablet},
	pages = {228--238},
	file = {2019 Understanding and Improving Regression Test Selection in Continuous Integration.pdf:/Users/renangreca/Zotero/storage/MGY6CNAS/2019 Understanding and Improving Regression Test Selection in Continuous Integration.pdf:application/pdf},
}

@article{haghighatkhah_test_2018,
	title = {Test prioritization in continuous integration environments},
	volume = {146},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121218301730},
	doi = {10.1016/j.jss.2018.08.061},
	language = {en},
	urldate = {2021-05-11},
	journal = {Journal of Systems and Software},
	author = {Haghighatkhah, Alireza and Mäntylä, Mika and Oivo, Markku and Kuvaja, Pasi},
	month = dec,
	year = {2018},
	keywords = {\_tablet},
	pages = {80--98},
	file = {2018 Test prioritization in continuous integration environments.pdf:/Users/renangreca/Zotero/storage/EAYEFMX3/2018 Test prioritization in continuous integration environments.pdf:application/pdf},
}

@inproceedings{chen_test_2016,
	address = {Chicago, IL, USA},
	title = {Test {Case} {Prioritization} for {Compilers}: {A} {Text}-{Vector} {Based} {Approach}},
	isbn = {978-1-5090-1827-7},
	shorttitle = {Test {Case} {Prioritization} for {Compilers}},
	url = {http://ieeexplore.ieee.org/document/7515478/},
	doi = {10.1109/ICST.2016.19},
	abstract = {Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efﬁciency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach ﬁrst transforms each test case into a text-vector by extracting its tokens which reﬂect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efﬁciency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and ﬁnd that our approach is much more efﬁcient than the existing approaches and is effective in prioritizing test cases.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	publisher = {IEEE},
	author = {Chen, Junjie and Bai, Yanwei and Hao, Dan and Xiong, Yingfei and Zhang, Hongyu and Zhang, Lu and Xie, Bing},
	month = apr,
	year = {2016},
	keywords = {\_tablet},
	pages = {266--277},
	file = {2016 Test Case Prioritization for Compilers.pdf:/Users/renangreca/Zotero/storage/TV3XFXIT/2016 Test Case Prioritization for Compilers.pdf:application/pdf},
}

@inproceedings{yu_terminator_2019,
	address = {Tallinn Estonia},
	title = {{TERMINATOR}: better automated {UI} test case prioritization},
	isbn = {978-1-4503-5572-8},
	shorttitle = {{TERMINATOR}},
	url = {https://dl.acm.org/doi/10.1145/3338906.3340448},
	doi = {10.1145/3338906.3340448},
	abstract = {Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page’s microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process. To mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is “black box” in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 “black box” test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Yu, Zhe and Fahid, Fahmid and Menzies, Tim and Rothermel, Gregg and Patrick, Kyle and Cherian, Snehit},
	month = aug,
	year = {2019},
	keywords = {\_tablet},
	pages = {883--894},
	file = {2019 TERMINATOR.pdf:/Users/renangreca/Zotero/storage/KC9A9PEV/2019 TERMINATOR.pdf:application/pdf},
}

@inproceedings{azizi_retest_2018,
	address = {Memphis, TN},
	title = {{ReTEST}: {A} {Cost} {Effective} {Test} {Case} {Selection} {Technique} for {Modern} {Software} {Development}},
	isbn = {978-1-5386-8321-7},
	shorttitle = {{ReTEST}},
	url = {https://ieeexplore.ieee.org/document/8539077/},
	doi = {10.1109/ISSRE.2018.00025},
	abstract = {Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modiﬁed version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel languageindependent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efﬁcient by selecting a far smaller subset of tests compared to the existing techniques.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2018 {IEEE} 29th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	publisher = {IEEE},
	author = {Azizi, Maral and Do, Hyunsook},
	month = oct,
	year = {2018},
	keywords = {\_tablet},
	pages = {144--154},
	file = {2018 ReTEST.pdf:/Users/renangreca/Zotero/storage/GBURVKD7/2018 ReTEST.pdf:application/pdf},
}

@inproceedings{fu_resurgence_2019,
	address = {Xi'an, China},
	title = {Resurgence of {Regression} {Test} {Selection} for {C}++},
	isbn = {978-1-72811-736-2},
	url = {https://ieeexplore.ieee.org/document/8730161/},
	doi = {10.1109/ICST.2019.00039},
	abstract = {Regression testing – running available tests after each project change – is widely practiced in industry. Despite its widespread use and importance, regression testing is a costly activity. Regression test selection (RTS) optimizes regression testing by selecting only tests affected by project changes. RTS has been extensively studied and several tools have been deployed in large projects. However, work on RTS over the last decade has mostly focused on languages with abstract computing machines (e.g., JVM). Meanwhile development practices (e.g., frequency of commits, testing frameworks, compilers) in C++ projects have dramatically changed and the way we should design and implement RTS tools and the beneﬁts of those tools is unknown. We present a design and implementation of an RTS technique, dubbed RTS++, that targets projects written in C++, which compile to LLVM IR and use the Google Test testing framework. RTS++ uses static analysis of a function call graph to select tests. RTS++ integrates with many existing build systems, including AutoMake, CMake, and Make. We evaluated RTS++ on 11 large open-source projects, totaling 3,811,916 lines of code. To the best of our knowledge, this is the largest evaluation of an RTS technique for C++. We measured the beneﬁts of RTS++ compared to running all available tests (i.e., retest-all). Our results show that RTS++ reduces the number of executed tests and end-to-end testing time by 88\% and 61\% on average.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	publisher = {IEEE},
	author = {Fu, Ben and Misailovic, Sasa and Gligoric, Milos},
	month = apr,
	year = {2019},
	keywords = {\_tablet},
	pages = {323--334},
	file = {2019 Resurgence of Regression Test Selection for C++.pdf:/Users/renangreca/Zotero/storage/RG7RWXL7/2019 Resurgence of Regression Test Selection for C++.pdf:application/pdf},
}

@inproceedings{dirim_prioritization_2020,
	address = {Porto, Portugal},
	title = {Prioritization of {Test} {Cases} with {Varying} {Test} {Costs} and {Fault} {Severities} for {Certification} {Testing}},
	isbn = {978-1-72811-075-2},
	url = {https://ieeexplore.ieee.org/document/9155581/},
	doi = {10.1109/ICSTW50294.2020.00069},
	abstract = {We present an industrial case study on the application of test case prioritization techniques in the context of certiﬁcation testing in consumer electronics domain. Test execution times and fault severities are subject to high variations in this domain. As a result, most of the existing techniques and metrics turn out to be inappropriate for this application context. We discuss such deﬁciencies and the room for improvement based on our case study with the certiﬁcation test suites of 3 Smart TV applications as real experimental objects. We also propose a new metric, LAPFD, which is based on the calculation of the average of the percentage of faults detected. This calculation is weighted according to the cost of test cases and calculated separately per severity class. Then, a lexicographic ordering is performed based on these classes. We compared the baseline (random) ordering of test cases with respect to an alternative ordering based on cost, measured as the test execution time. These alternative orderings are evaluated by using the LAPFD metric. We observed that costbased ordering of test cases consistently outperformed random ordering. Another observation is that there is a large room for improvement regarding the effectiveness of test case prioritization in this application domain.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	publisher = {IEEE},
	author = {Dirim, Sahin and Sozer, Hasan},
	month = oct,
	year = {2020},
	keywords = {\_tablet},
	pages = {386--391},
	file = {2020 Prioritization of Test Cases with Varying Test Costs and Fault Severities for.pdf:/Users/renangreca/Zotero/storage/HXWZFEJ4/2020 Prioritization of Test Cases with Varying Test Costs and Fault Severities for.pdf:application/pdf},
}

@article{garousi_multi-objective_2018,
	title = {Multi-objective regression test selection in practice: {An} empirical study in the defense software industry},
	volume = {103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049450827&doi=10.1016%2fj.infsof.2018.06.007&partnerID=40&md5=8a07f1121c7e1b25d38319437906c4ad},
	doi = {10.1016/j.infsof.2018.06.007},
	abstract = {Context: Executing an entire regression test-suite after every code change is often costly in large software projects. To cope with this challenge, researchers have proposed various regression test-selection techniques. Objective: This paper was motivated by a real industrial need to improve regression-testing practices in the context of a safety-critical industrial software in the defence domain in Turkey. To address our objective, we set up and conducted an “action-research” collaborative project between industry and academia. Method: After a careful literature review, we selected a conceptual multi-objective regression-test selection framework (called MORTO) and adopted it to our industrial context by developing a custom-built genetic algorithm (GA) based on that conceptual framework. GA is able to provide full coverage of the affected (changed) requirements while considering multiple cost and benefit factors of regression testing. e.g., minimizing the number of test cases, and maximizing cumulative number of detected faults by each test suite. Results: The empirical results of applying the approach on the Software Under Test (SUT) demonstrate that this approach yields a more efficient test suite (in terms of costs and benefits) compared to the old (manual) test-selection approach, used in the company, and another applicable approach chosen from the literature. With this new approach, regression selection process in the project under study is not ad-hoc anymore. Furthermore, we have been able to eliminate the subjectivity of regression testing and its dependency on expert opinions. Conclusion: Since the proposed approach has been beneficial in saving the costs of regression testing, it is currently in active use in the company. We believe that other practitioners can apply our approach in their regression-testing contexts too, when applicable. Furthermore, this paper contributes to the body of evidence in regression testing by offering a success story of successful implementation and application of multi-objective regression testing in practice. © 2018},
	journal = {Information and Software Technology},
	author = {Garousi, V. and Özkan, R. and Betin-Can, A.},
	year = {2018},
	keywords = {\_tablet},
	pages = {40--54},
	file = {2018 Multi-objective regression test selection in practice.pdf:/Users/renangreca/Zotero/storage/5TVJFYAW/2018 Multi-objective regression test selection in practice.pdf:application/pdf},
}

@inproceedings{lu_how_2016,
	address = {Austin Texas},
	title = {How does regression test prioritization perform in real-world software evolution?},
	isbn = {978-1-4503-3900-1},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884874},
	doi = {10.1145/2884781.2884874},
	abstract = {In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little e↵ort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artiﬁcial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artiﬁcial changes are still valid for real-world software evolution. In this paper, we present the ﬁrst empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation signiﬁcantly hampers their e↵ectiveness, whereas source code changes alone do not inﬂuence their e↵ectiveness much.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Lu, Yafeng and Lou, Yiling and Cheng, Shiyang and Zhang, Lingming and Hao, Dan and Zhou, Yangfan and Zhang, Lu},
	month = may,
	year = {2016},
	keywords = {\_tablet},
	pages = {535--546},
	file = {2016 How does regression test prioritization perform in real-world software evolution.pdf:/Users/renangreca/Zotero/storage/LPAK5A3Q/2016 How does regression test prioritization perform in real-world software evolution.pdf:application/pdf},
}

@inproceedings{zhang_hybrid_2018,
	address = {Gothenburg Sweden},
	title = {Hybrid regression test selection},
	isbn = {978-1-4503-5638-1},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180198},
	doi = {10.1145/3180155.3180198},
	abstract = {Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Zhang, Lingming},
	month = may,
	year = {2018},
	keywords = {\_tablet},
	pages = {199--209},
	file = {2018 Hybrid regression test selection.pdf:/Users/renangreca/Zotero/storage/ZST45KQR/2018 Hybrid regression test selection.pdf:application/pdf},
}

@inproceedings{vasic_file-level_2017,
	address = {Paderborn Germany},
	title = {File-level vs. module-level regression test selection for .{NET}},
	isbn = {978-1-4503-5105-8},
	url = {https://dl.acm.org/doi/10.1145/3106237.3117763},
	doi = {10.1145/3106237.3117763},
	abstract = {Regression testing is used to check the correctness of evolving software. With the adoption of Agile development methodology, the number of tests and software revisions has dramatically increased, and hence has the cost of regression testing. Researchers proposed regression test selection (RTS) techniques that optimize regression testing by skipping tests that are not impacted by recent program changes. Ekstazi is one such state-of-the art technique; Ekstazi is implemented for the Java programming language and has been adopted by several companies and open-source projects.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Vasic, Marko and Parvez, Zuhair and Milicevic, Aleksandar and Gligoric, Milos},
	month = aug,
	year = {2017},
	keywords = {\_tablet},
	pages = {848--853},
	file = {2017 File-level vs.pdf:/Users/renangreca/Zotero/storage/NICEBXU3/2017 File-level vs.pdf:application/pdf},
}

@inproceedings{marijan_effect_2016,
	address = {Raleigh, NC, USA},
	title = {Effect of {Time} {Window} on the {Performance} of {Continuous} {Regression} {Testing}},
	isbn = {978-1-5090-3806-0},
	url = {http://ieeexplore.ieee.org/document/7816510/},
	doi = {10.1109/ICSME.2016.77},
	abstract = {Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at ﬁnding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our ﬁndings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Marijan, Dusica and Liaaen, Marius},
	month = oct,
	year = {2016},
	keywords = {\_tablet},
	pages = {568--571},
	file = {2016 Effect of Time Window on the Performance of Continuous Regression Testing.pdf:/Users/renangreca/Zotero/storage/2CUPB99V/2016 Effect of Time Window on the Performance of Continuous Regression Testing.pdf:application/pdf},
}

@inproceedings{peng_empirically_2020,
	address = {Virtual Event USA},
	title = {Empirically revisiting and enhancing {IR}-based test-case prioritization},
	isbn = {978-1-4503-8008-9},
	url = {https://dl.acm.org/doi/10.1145/3395363.3397383},
	doi = {10.1145/3395363.3397383},
	abstract = {Test-case prioritization (TCP) aims to detect regression bugs faster via reordering the tests run. While TCP has been studied for over 20 years, it was almost always evaluated using seeded faults/mutants as opposed to using real test failures. In this work, we study the recent change-aware information retrieval (IR) technique for TCP. Prior work has shown it performing better than traditional coverage-based TCP techniques, but it was only evaluated on a small-scale dataset with a cost-unaware metric based on seeded faults/mutants. We extend the prior work by conducting a much larger and more realistic evaluation as well as proposing enhancements that substantially improve the performance. In particular, we evaluate the original technique on a large-scale, real-world software-evolution dataset with real failures using both cost-aware and cost-unaware metrics under various configurations. Also, we design and evaluate hybrid techniques combining the IR features, historical test execution time, and test failure frequencies. Our results show that the change-aware IR technique outperforms stateof-the-art coverage-based techniques in this real-world setting, and our hybrid techniques improve even further upon the original IR technique. Moreover, we show that flaky tests have a substantial impact on evaluating the change-aware TCP techniques based on real test failures.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Peng, Qianyang and Shi, August and Zhang, Lingming},
	month = jul,
	year = {2020},
	keywords = {\_tablet},
	pages = {324--336},
	file = {2020 Empirically revisiting and enhancing IR-based test-case prioritization.pdf:/Users/renangreca/Zotero/storage/H7ZUB6IN/2020 Empirically revisiting and enhancing IR-based test-case prioritization.pdf:application/pdf},
}

ser

@inproceedings{tahvili_dynamic_2016,
	address = {Chicago, IL, USA},
	title = {Dynamic {Integration} {Test} {Selection} {Based} on {Test} {Case} {Dependencies}},
	isbn = {978-1-5090-3674-5},
	url = {http://ieeexplore.ieee.org/document/7528974/},
	doi = {10.1109/ICSTW.2016.14},
	abstract = {Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unﬁt for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {2016 {IEEE} {Ninth} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	publisher = {IEEE},
	author = {Tahvili, Sahar and Saadatmand, Mehrdad and Larsson, Stig and Afzal, Wasif and Bohlin, Markus and Sundmark, Daniel},
	month = apr,
	year = {2016},
	keywords = {\_tablet},
	pages = {277--286},
	file = {2016 Dynamic Integration Test Selection Based on Test Case Dependencies.pdf:/Users/renangreca/Zotero/storage/ZAXMV2PE/2016 Dynamic Integration Test Selection Based on Test Case Dependencies.pdf:application/pdf},
}

@article{schwartz_cost-effective_2016,
	title = {Cost-effective regression testing through {Adaptive} {Test} {Prioritization} strategies},
	volume = {115},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216000169},
	doi = {10.1016/j.jss.2016.01.018},
	abstract = {Regression testing is an important part of the software development life cycle. It is also very expensive. Many different techniques have been proposed for reducing the cost of regression testing. However, research has shown that the effectiveness of different techniques varies under different testing environments and software change characteristics. In prior work, we developed strategies to investigate ways of choosing the most cost-effective regression testing technique for a particular regression testing session. In this work, we empirically study the existing strategies presented in prior work as well as develop two additional Adaptive Test Prioritization (ATP) strategies using fuzzy analytical hierarchy process (AHP) and the weighted sum model (WSM). We also provide a comparative study examining each of the ATP strategies presented to date. This research will provide researchers and practitioners with strategies to utilize in regression testing plans as well as provide data to use when deciding which of the strategies would best ﬁt their testing needs. The empirical studies provided in this research show that utilizing these strategies can improve the cost-effectiveness of regression testing.},
	language = {en},
	urldate = {2021-05-11},
	journal = {Journal of Systems and Software},
	author = {Schwartz, Amanda and Do, Hyunsook},
	month = may,
	year = {2016},
	keywords = {\_tablet},
	pages = {61--81},
	file = {2016 Cost-effective regression testing through Adaptive Test Prioritization.pdf:/Users/renangreca/Zotero/storage/JWX4TPDS/2016 Cost-effective regression testing through Adaptive Test Prioritization.pdf:application/pdf},
}

@article{zhou_beating_2020,
	title = {Beating {Random} {Test} {Case} {Prioritization}},
	issn = {0018-9529, 1558-1721},
	url = {https://ieeexplore.ieee.org/document/9118977/},
	doi = {10.1109/TR.2020.2979815},
	abstract = {Existing test case prioritization (TCP) techniques have limitations when applied to real-world projects, because these techniques require certain information to be made available before they can be applied. For example, the family of input-based TCP techniques are based on test case values or test script strings; other techniques use test coverage, test history, program structure, or requirements information. Existing techniques also cannot guarantee to always be more effective than random prioritization (RP) that does not have any precondition. As a result, RP remains the most applicable and most fundamental TCP technique. This article proposes an extremely simple, effective, and efﬁcient way to prioritize test cases through the introduction of a dispersity metric. Our technique is as applicable as RP. We conduct empirical studies using 43 different versions of 15 real-world projects. Empirical results show that our technique is more effective than RP. Our algorithm has a linear computational complexity and, therefore, provides a practical solution to the problem of prioritizing very large test suites (such as those containing hundreds of thousands, or millions, of test cases), where the execution time of conventional nonlinear prioritization algorithms can be prohibitive. Our technique also provides a practical solution to TCP when neither input-based nor execution-based techniques are applicable due to lack of information.},
	language = {en},
	urldate = {2021-05-11},
	journal = {IEEE Transactions on Reliability},
	author = {Zhou, Zhi Quan and Liu, Chen and Chen, Tsong Yueh and Tse, T. H. and Susilo, Willy},
	year = {2020},
	keywords = {\_tablet},
	pages = {1--22},
	file = {2020 Beating Random Test Case Prioritization.pdf:/Users/renangreca/Zotero/storage/MCEB6WM3/2020 Beating Random Test Case Prioritization.pdf:application/pdf},
}

@inproceedings{magalhaes_automatic_2016,
	address = {Maringa, Parana, Brazil},
	title = {Automatic {Selection} of {Test} {Cases} for {Regression} {Testing}},
	isbn = {978-1-4503-4766-2},
	url = {http://dl.acm.org/citation.cfm?doid=2993288.2993299},
	doi = {10.1145/2993288.2993299},
	abstract = {Regression testing is a safety measure to attest that changes made on a system preserve prior accepted behavior. Identifying which test cases must compose a regression test suite in a certain development stage is tricky, particularly when one only has test cases and change requests described in natural language, and the execution of the test suite will be performed manually. That is the case of our industrial partner. We propose a selection of regression test cases based on information retrieval and implement as a web-service. In performed experiments, we show that we can improve the creation of regression test suites of our industrial partner by providing more eﬀective test cases based on keywords analysis in an automatic way.},
	language = {en},
	urldate = {2021-05-11},
	booktitle = {Proceedings of the 1st {Brazilian} {Symposium} on {Systematic} and {Automated} {Software} {Testing} - {SAST}},
	publisher = {ACM Press},
	author = {Magalhães, Cláudio and Barros, Flávia and Mota, Alexandre and Maia, Eliot},
	year = {2016},
	keywords = {\_tablet},
	pages = {1--8},
	file = {2016 Automatic Selection of Test Cases for Regression Testing.pdf:/Users/renangreca/Zotero/storage/WQY3EF25/2016 Automatic Selection of Test Cases for Regression Testing.pdf:application/pdf},
}

@inproceedings{land_industrial_2019,
	address = {Bari, Italy},
	title = {An {Industrial} {Evaluation} of {Test} {Prioritisation} {Criteria} and {Metrics}},
	isbn = {978-1-72814-569-3},
	url = {https://ieeexplore.ieee.org/document/8914505/},
	doi = {10.1109/SMC.2019.8914505},
	urldate = {2021-05-11},
	booktitle = {2019 {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics} ({SMC})},
	publisher = {IEEE},
	author = {Land, Kathrin and Neumann, Eva-Maria and Ziegltrum, Simon and Li, Huaxia and Vogel-Heuser, Birgit},
	month = oct,
	year = {2019},
	keywords = {\_tablet},
	pages = {1887--1892},
	file = {2019 An Industrial Evaluation of Test Prioritisation Criteria and Metrics.pdf:/Users/renangreca/Zotero/storage/C9J22KJD/2019 An Industrial Evaluation of Test Prioritisation Criteria and Metrics.pdf:application/pdf},
}

@article{eda_efficient_2019,
	title = {An efficient regression testing approach for {PHP} {Web} applications using test selection and reusable constraints},
	volume = {27},
	issn = {0963-9314, 1573-1367},
	url = {http://link.springer.com/10.1007/s11219-019-09449-2},
	doi = {10.1007/s11219-019-09449-2},
	abstract = {Web applications undergo frequent changes. These changes can be due to the addition of new features or the modification of existing features to support customer requests or to patch faults in the system. Given that Web applications have a large surface area subject to attack, changes often include security fixes either in response to malicious attacks or to forestall such attacks. Effective regression testing should ensure that any change does not disable existing features or compromise security. Executing the entire regression test suite takes time and consumes many resources. One approach is to focus regression test efforts only on code paths that were modified in the new version. Such code paths can be identified using tools such as PHP Analysis and Regression Testing Engine (PARTE). In this paper, we extend this approach to test selection where a subset of existing tests that cover the modified code paths can be detected. To further reduce the amount of regression testing needed, we used PARTE’s reusable constraint value information to identify tests that can be reused against the new version without having to modify the input test values. We performed an empirical study to determine whether test selection data combined with reusable constraint values would further improve the turnaround time for regression tests. Results from the experiment conducted on four Hypertext Preprocessor (PHP) web applications demonstrate that this approach is effective in reducing the cost of regression testing of frequently patched Web applications.},
	language = {en},
	number = {4},
	urldate = {2021-05-11},
	journal = {Software Quality Journal},
	author = {Eda, Ravi and Do, Hyunsook},
	month = dec,
	year = {2019},
	keywords = {\_tablet},
	pages = {1383--1417},
	file = {2019 An efficient regression testing approach for PHP Web applications using test.pdf:/Users/renangreca/Zotero/storage/SICEPAPU/2019 An efficient regression testing approach for PHP Web applications using test.pdf:application/pdf},
}

@inproceedings{zhu_test_2018,
	title = {Test re-prioritization in continuous testing environments},
	isbn = {978-1-5386-7870-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058303599&doi=10.1109%2FICSME.2018.00016&partnerID=40&md5=e2aa1f518ab3b20b5d34ba61f91fd93d},
	doi = {10.1109/ICSME.2018.00016},
	abstract = {New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31\% and 62\% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27\% faster and all failures 62\% faster. © 2018 IEEE.},
	booktitle = {Proceedings - 2018 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution}, {ICSME} 2018},
	publisher = {IEEE},
	author = {Zhu, Yuecai and Shihab, Emad and Rigby, Peter C.},
	year = {2018},
	keywords = {\_tablet, Regression testing, Continuous integration, Continuous testing, Dynamic test prioritization, Test dependency, Test minimization},
	pages = {69--79},
	file = {2018 Test re-prioritization in continuous testing environments.pdf:/Users/renangreca/Zotero/storage/6QF2X4XP/2018 Test re-prioritization in continuous testing environments.pdf:application/pdf},
}

@inproceedings{goyal_test_2019,
	title = {Test suite minimization of evolving software systems: {A} case study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073110661&partnerID=40&md5=382e58eacfab1d21bbfba93834bcbd87},
	abstract = {Test suite minimization ensures that an optimum set of test cases are selected to provide maximum coverage of requirements. In this paper, we discuss and evaluate techniques for test suite minimization of evolving software systems. As a case study, we have used an industrial tool, Static Code Analysis (SCAN) tool for Electronic Device Description Language (EDDL) as the System Under Test (SUT). We have used standard approaches including Greedy, Greedy Essential (GE) and Greedy Redundant Essential (GRE) for minimization of the test suite for a given set of requirements of the SUT. Further, we have proposed and implemented k-coverage variants of these approaches. The minimized test suite which is obtained as a result reduces testing effort and time during regression testing. The paper also addresses the need for choosing an appropriate level of granularity of requirements to efficiently cover all requirements. The paper demonstrates how fine grained requirements help in finding an optimal test suite to completely address the requirements and also help in detecting bugs in each version of the software. Finally, the results from different analyses have been presented and compared and it has been observed that GE heuristics performs the best (run time) under certain conditions. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	booktitle = {{ICSOFT} 2019 - {Proceedings} of the 14th {International} {Conference} on {Software} {Technologies}},
	author = {Goyal, A. and Shyamasundar, R.K. and Jetley, R. and Mohan, D. and Ramaswamy, S.},
	year = {2019},
	keywords = {\_tablet},
	pages = {226--237},
	file = {2019 Test suite minimization of evolving software systems.pdf:/Users/renangreca/Zotero/storage/7B29FK92/2019 Test suite minimization of evolving software systems.pdf:application/pdf},
}

@inproceedings{vost_trace-based_2016,
	title = {Trace-based test selection to support continuous integration in the automotive industry},
	isbn = {978-1-4503-4157-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984645399&doi=10.1145%2F2896941.2896951&partnerID=40&md5=b51094be93307059bba02aa265d91b6c},
	doi = {10.1145/2896941.2896951},
	abstract = {System testing in the automotive industry is a very expensive and time-consuming task of growing importance, because embedded systems in the domain are distributed over numerous controllers (ECUs). Modern software development techniques such as continuous integration require regular, repeated and fast testing. To achieve this in the automotive domain, test suites for a specific software change must be tailored. We propose a novel test selection technique for system-level functions in the automotive industry based on component and communication models. The idea is to follow input and output signals that are used in the testing steps through the ECUs implementing a function. We select only those tests for a planned integration in which at least one of the signals sent in its steps is processed by the ECU that was changed and thus triggered the integration. The technique is well-suited for black-box testing since it requires only the full test suite specification and the system architecture. We applied the technique to a test suite of the Active Cruise Control function at BMW Group in the context of hardware-in-the-loop system testing and found the possible reduction rates to be 82.3\% on average in comparison to the full test suite. Possible future work includes the evaluation with a wider set of functions, the evaluation of the fault detection rate, further automation and combination with other test selection techniques.},
	booktitle = {Proceedings - {International} {Workshop} on {Continuous} {Software} {Evolution} and {Delivery}, {CSED} 2016},
	publisher = {Association for Computing Machinery, Inc},
	author = {Vöst, Sebastian and Wagner, Stefan},
	month = may,
	year = {2016},
	keywords = {\_tablet, Test case selection, Continuous integration, System testing, Embedded systems},
	pages = {34--40},
	file = {2016 Trace-based test selection to support continuous integration in the automotive.pdf:/Users/renangreca/Zotero/storage/45ZNQZVX/2016 Trace-based test selection to support continuous integration in the automotive.pdf:application/pdf},
}

@inproceedings{wang_enhancing_2016,
	title = {Enhancing test case prioritization in an industrial setting with resource awareness and multi-objective search},
	isbn = {978-1-4503-4161-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992463531&doi=10.1145%2F2889160.2889240&partnerID=40&md5=be139f2ccd182b7b778d3503ef0b0760},
	doi = {10.1145/2889160.2889240},
	abstract = {Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource-aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6\% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9\% and fault detection on average by 60\%.},
	booktitle = {Proceedings - {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Wang, Shuai and Ali, Shaukat and Yue, Tao and Bakkeli, Oyvind and Liaaen, Marius},
	month = may,
	year = {2016},
	keywords = {\_tablet, Test case prioritization, Multi-objective optimization, Search},
	pages = {182--191},
	file = {2016 Enhancing test case prioritization in an industrial setting with resource.pdf:/Users/renangreca/Zotero/storage/93I6D5F7/2016 Enhancing test case prioritization in an industrial setting with resource.pdf:application/pdf},
}

@inproceedings{yilmaz_case_2018,
	title = {A case study to compare regression test selection techniques on open-source software projects},
	volume = {2201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053685255&partnerID=40&md5=7a9a3a709ece8295bef3e46c9aeb10c7},
	abstract = {Regression testing is the type of testing performed on a modified software to validate integrated parts are functioning properly. Especially with agile development practices being increasingly used, regression testing needs to be fast and practical enough to coexist with the nature of agile development. To satisfy this need, Regression Test Selection (RTS) techniques are proposed to reduce number of tests. Although there are many studies analyzing these techniques and their effectiveness in terms of the number of reduced tests, time and cost; the applicability and practicality aspects have been mostly neglected. To this end, in this paper a case study is carried out to compare highly cited and mostly used RTS techniques. Selected techniques are applied on extensively used and tested open-source software projects, and the result of their comparison in regards of practicality, applicability, performance and cost-effectiveness are discussed.},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	author = {Yilmaz, U. and Tarhan, A.},
	year = {2018},
	keywords = {\_tablet},
	file = {2018 A case study to compare regression test selection techniques on open-source.pdf:/Users/renangreca/Zotero/storage/4PX64JAN/2018 A case study to compare regression test selection techniques on open-source.pdf:application/pdf},
}

@inproceedings{pradhan_search-based_2016,
	address = {New York, NY, USA},
	series = {{GECCO} '16},
	title = {Search-{Based} {Cost}-{Effective} {Test} {Case} {Selection} within a {Time} {Budget}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-4206-3},
	url = {https://biblioproxy.cnr.it:2481/10.1145/2908812.2908850},
	doi = {10.1145/2908812.2908850},
	abstract = {Due to limited time and resources available for execution, test case selection always remains crucial for cost-effective testing. It is even more prominent when test cases require manual steps, e.g., operating physical equipment. Thus, test case selection must consider complicated trade-offs between cost (e.g., execution time) and effectiveness (e.g., fault detection capability). Based on our industrial collaboration within the Maritime domain, we identified a real-world and multi-objective test case selection problem in the context of robustness testing, where test case execution requires human involvement in certain steps, such as turning on the power supply to a device. The high-level goal is to select test cases for execution within a given time budget, where test engineers provide weights for a set of objectives, depending on testing requirements, standards, and regulations. To address the identified test case selection problem, we defined a fitness function including one cost measure, i.e., Time Difference (TD) and three effectiveness measures, i.e., Mean Priority (MPR), Mean Probability (MPO) and Mean Consequence (MC) that were identified together with test engineers. We further empirically evaluated eight multi-objective search algorithms, which include three weight-based search algorithms (e.g., Alternating Variable Method) and five Pareto-based search algorithms (e.g., Strength Pareto Evolutionary Algorithm 2 (SPEA2)) using two weight assignment strategies (WASs). Notice that Random Search (RS) was used as a comparison baseline. We conducted two sets of empirical evaluations: 1) Using a real world case study that was developed based on our industrial collaboration; 2) Simulating the real world case study to a larger scale to assess the scalability of the search algorithms. Results show that SPEA2 with either of the WASs performed the best for both the studies. Overall, SPEA2 managed to improve on average 32.7\%, 39\% and 33\% in terms of MPR, MPO and MC respectively as compared to RS.},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} 2016},
	publisher = {Association for Computing Machinery},
	author = {Pradhan, Dipesh and Wang, Shuai and Ali, Shaukat and Yue, Tao},
	month = jul,
	year = {2016},
	keywords = {\_tablet, Test case selection, Search, multi-objective optimization, Multi-Objective optimization, search, test case selection},
	pages = {1085--1092},
	file = {2016 Search-Based Cost-Effective Test Case Selection within a Time Budget.pdf:/Users/renangreca/Zotero/storage/2CYTRAWE/2016 Search-Based Cost-Effective Test Case Selection within a Time Budget.pdf:application/pdf},
}

@inproceedings{cruciani_scalable_2019,
	address = {Montreal, QC, Canada},
	title = {Scalable {Approaches} for {Test} {Suite} {Reduction}},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812048/},
	doi = {10.1109/ICSE.2019.00055},
	abstract = {Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or speciﬁcation models, that are not commonly available at large scale. We present a family of novel very efﬁcient approaches for similaritybased test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for ﬁnding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input). We evaluate four approaches in a version that selects a ﬁxed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some ﬁxed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efﬁciency. When applied to a suite of more than 500K real world test cases, the most efﬁcient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.},
	language = {en},
	urldate = {2019-09-06},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Cruciani, Emilio and Miranda, Breno and Verdecchia, Roberto and Bertolino, Antonia},
	month = may,
	year = {2019},
	keywords = {\_tablet, Software testing, Test suite reduction, Clustering, Random projection, Similarity-based testing},
	pages = {419--429},
	file = {2019 Scalable Approaches for Test Suite Reduction.pdf:/Users/renangreca/Zotero/storage/TBM2Q2SL/2019 Scalable Approaches for Test Suite Reduction.pdf:application/pdf},
}

@inproceedings{miranda_fast_2018,
	address = {Gothenburg, Sweden},
	title = {{FAST} approaches to scalable similarity-based test case prioritization},
	isbn = {978-1-4503-5638-1},
	url = {http://dl.acm.org/citation.cfm?doid=3180155.3180210},
	doi = {10.1145/3180155.3180210},
	abstract = {Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similaritybased approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to ﬁnd similar items. FAST techniques provide scalable similaritybased test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in eﬃciency with no signiﬁcant impact on eﬀectiveness, and also outperform whitebox approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.},
	language = {en},
	urldate = {2019-09-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}  - {ICSE} '18},
	publisher = {ACM Press},
	author = {Miranda, Breno and Cruciani, Emilio and Verdecchia, Roberto and Bertolino, Antonia},
	year = {2018},
	keywords = {\_tablet},
	pages = {222--232},
	file = {2018 FAST approaches to scalable similarity-based test case prioritization.pdf:/Users/renangreca/Zotero/storage/SV5ESE4H/2018 FAST approaches to scalable similarity-based test case prioritization.pdf:application/pdf;Miranda et al_2018_FAST Approaches to Scalable Similarity-Based Test Case Prioritization.pdf:/Users/renangreca/Dropbox/Zotero/Miranda et al_2018_FAST Approaches to Scalable Similarity-Based Test Case Prioritization.pdf:application/pdf;Miranda et al_2018_FAST approaches to scalable similarity-based test case prioritization.pdf:/Users/renangreca/Zotero/storage/HPBMQZNV/Miranda et al_2018_FAST approaches to scalable similarity-based test case prioritization.pdf:application/pdf},
}

@inproceedings{celik_regression_2017,
	title = {Regression test selection across {JVM} boundaries},
	volume = {Part F130154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030784137&doi=10.1145%2f3106237.3106297&partnerID=40&md5=686a0699d1a765cbfe7a3d3048ab8cf9},
	doi = {10.1145/3106237.3106297},
	abstract = {Modern software development processes recommend that changes be integrated into the main development line of a project multiple times a day. Before a new revision may be integrated, developers practice regression testing to ensure that the latest changes do not break any previously established functionality. The cost of regression testing is high, due to an increase in the number of revisions that are introduced per day, as well as the number of tests developers write per revision. Regression test selection (RTS) optimizes regression testing by skipping tests that are not affected by recent project changes. Existing dynamic RTS techniques support only projects written in a single programming language, which is unfortunate knowing that an open-source project is on average written in several programming languages. We present the first dynamic RTS technique that does not stop at predefined language boundaries. Our technique dynamically detects, at the operating system level, all file artifacts a test depends on. Our technique is, hence, oblivious to the specific means the test uses to actually access the files: be it through spawning a new process, invoking a system call, invoking a library written in a different language, invoking a library that spawns a process which makes a system call, etc. We also provide a set of extension points which allow for a smooth integration with testing frameworks and build systems. We implemented our technique in a tool called RTSLinux as a loadable Linux kernel module and evaluated it on 21 Java projects that escape the JVM by spawning new processes or invoking native code, totaling 2, 050, 791 lines of code. Our results show that RTSLinux, on average, skips 74.17\% of tests and saves 52.83\% of test execution time compared to executing all tests. © 2017 Association for Computing Machinery.},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Celik, A. and Vasic, M. and Milicevic, A. and Gligoric, M.},
	year = {2017},
	keywords = {\_tablet, Regression test selection, Language-agnostic},
	pages = {809--820},
	file = {2017 Regression test selection across JVM boundaries.pdf:/Users/renangreca/Zotero/storage/HF39GP8U/2017 Regression test selection across JVM boundaries.pdf:application/pdf},
}

@inproceedings{chen_optimizing_2018,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2018},
	title = {Optimizing {Test} {Prioritization} via {Test} {Distribution} {Analysis}},
	isbn = {978-1-4503-5573-5},
	url = {https://biblioproxy.cnr.it:2481/10.1145/3236024.3236053},
	doi = {10.1145/3236024.3236053},
	abstract = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice. To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming stateof- the-art techniques significantly in regression testing time, e.g., 43.16\% to 94.92\% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks fromthe testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Junjie and Lou, Yiling and Zhang, Lingming Lu and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lingming Lu},
	month = oct,
	year = {2018},
	keywords = {\_tablet, Regression Testing, Machine Learning, e.g., saving beyond 2X testing costs with negligible ove, Test Prioritization, the testing team of this company},
	pages = {656--667},
	file = {2018 Optimizing Test Prioritization via Test Distribution Analysis.pdf:/Users/renangreca/Zotero/storage/W56PBEMF/2018 Optimizing Test Prioritization via Test Distribution Analysis.pdf:application/pdf},
}

@inproceedings{philip_fastlane:_2019,
	address = {Montreal, QC, Canada},
	title = {{FastLane}: {Test} {Minimization} for {Rapidly} {Deployed} {Large}-{Scale} {Online} {Services}},
	isbn = {978-1-72810-869-8},
	shorttitle = {{FastLane}},
	url = {https://ieeexplore.ieee.org/document/8812033/},
	doi = {10.1109/ICSE.2019.00054},
	abstract = {Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and ﬁxes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases.},
	language = {en},
	urldate = {2019-12-09},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Philip, Adithya Abraham and Bhagwan, Ranjita and Kumar, Rahul and Maddila, Chandra Sekhar and Nagppan, Nachiappan},
	month = may,
	year = {2019},
	keywords = {\_tablet, industry, microsoft, test prioritization, commit risk, machine learning},
	pages = {408--418},
	file = {2019 FastLane.pdf:/Users/renangreca/Zotero/storage/2UW93MR9/2019 FastLane.pdf:application/pdf},
}

@article{guo_decomposing_2019,
	title = {Decomposing {Composite} {Changes} for {Code} {Review} and {Regression} {Test} {Selection} in {Evolving} {Software}},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063625583&doi=10.1007%2fs11390-019-1917-9&partnerID=40&md5=202f466788254dfaaf0026d69e9a916a},
	doi = {10.1007/s11390-019-1917-9},
	abstract = {Inspecting and testing code changes typically require a significant amount of developer effort. As a system evolves, developers often create composite changes by mixing multiple development issues, as opposed to addressing one independent issue --- an atomic change. Inspecting composite changes often becomes time-consuming and error-prone. To test unrelated edits on composite changes, rerunning all regression tests may require excessive time. To address the problem, we present an interactive technique for change decomposition to support code reviews and regression test selection, called ChgCutter. When a developer specifies code change within a diff patch, ChgCutter partitions composite changes into a set of related atomic changes, which is more cohesive and self-contained regarding the issue being addressed. For composite change inspection, it generates an intermediate program version that only includes a related change subset using program dependence relationships. For cost reduction during regression testing, it safely selects only affected tests responsible for changes to an intermediate version. In the evaluation, we apply ChgCutter to 28 composite changes in four open source projects. ChgCutter partitions these changes with 95.7\% accuracy, while selecting affected tests with 89.0\% accuracy. We conduct a user study with professional software engineers at PayPal and find that ChgCutter is helpful in understanding and validating composite changes, scaling to industry projects. © 2019, Springer Science+Business Media, LLC \& Science Press, China.},
	number = {2},
	journal = {Journal of Computer Science and Technology},
	author = {Guo, B. and Kwon, Y.-W. and Song, M.},
	year = {2019},
	keywords = {\_tablet},
	pages = {416--436},
	file = {2019 Decomposing Composite Changes for Code Review and Regression Test Selection in.pdf:/Users/renangreca/Zotero/storage/6FFCVNCE/2019 Decomposing Composite Changes for Code Review and Regression Test Selection in.pdf:application/pdf},
}

@inproceedings{kwon_cost-effective_2017,
  title={Cost-effective regression testing using bloom filters in continuous integration development environments},
  author={Kwon, Jung-Hyun and Ko, In-Young},
  booktitle={2017 24th Asia-Pacific Software Engineering Conference (APSEC)},
  pages={160--168},
  year={2017},
  organization={IEEE}
}

@inproceedings{yackley_simultaneous_2019,
	title = {Simultaneous refactoring and regression testing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077812942&doi=10.1109%2fSCAM.2019.00032&partnerID=40&md5=ae64d65b5ce1a8abb8873dc8fc9a7544},
	doi = {10.1109/SCAM.2019.00032},
	abstract = {Currently, refactoring and regression testing are treated independently by existing studies. However, software developers frequently switch between these two activities, using regression testing to identify unwanted behavior changes introduced while refactoring and applying refactoring on identified buggy code fragments. Our hypothesis is that the tools to support developers in these two tasks could transfer part of the knowledge extracted from the process of finding refactoring opportunities to identify relevant test cases, and vice-versa. We propose a simultasking, search-based algorithm that unifies the tasks of refactoring and regression testing, hence solving them simultaneously and enabling knowledge transfer between them. The salient feature of the proposed algorithm is a unified and generic solution representation scheme for both problems, which serves as a common platform for knowledge transfer between them. We implemented and evaluated the proposed simultasking approach on six opensource systems and one industrial project. Our study features quantitative and qualitative analysis performed with developers, and the results achieved show that the proposed approach provides advantages over mono-task techniques treating refactoring and regression testing separately. © 2019 IEEE.},
	booktitle = {Proceedings - 19th {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}, {SCAM} 2019},
	author = {Yackley, J.J. and Kessentini, M. and Bavota, G. and Alizadeh, V. and Maxim, B.R.},
	year = {2019},
	keywords = {\_tablet, Regression testing, Refactoring, Search based software engineering},
	pages = {216--227},
	file = {2019 Simultaneous refactoring and regression testing.pdf:/Users/renangreca/Zotero/storage/BTUGUFGN/2019 Simultaneous refactoring and regression testing.pdf:application/pdf},
}

@inproceedings{yoshida_fsx_2016,
	title = {{FSX}: {A} tool for fine-grained incremental unit test generation for {C}/{C}++ {Programs}},
	volume = {13-18-November-2016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997272247&doi=10.1145%2f2950290.2983937&partnerID=40&md5=bab52e80bcf4d0434e3361bf7900dd20},
	doi = {10.1145/2950290.2983937},
	abstract = {Automated unit test generation bears the promise of significantly reducing test cost and hence improving software quality. However, the maintenance cost of the automatically generated tests presents a significant barrier to adoption of this technology. To address this challenge, in previous work, we proposed a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation of an existing test suite. In this paper we describe a tool FSX, implementing this technique. We describe the architecture, user-interface, and salient features of FSX, and specific practical use-cases of its technology. We also report on a real, large-scale deployment of FSX as a practical validation of the underlying research contribution and of automated test generation research in general. © 2016 ACM.},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} {Symposium} on the {Foundations} of {Software} {Engineering}},
	author = {Yoshida, H. and Tokumoto, S. and Prasad, M.R. and Ghosh, I. and Uehara, T.},
	year = {2016},
	keywords = {\_tablet},
	pages = {1052--1056},
	file = {2016 FSX.pdf:/Users/renangreca/Zotero/storage/3MZFF4G3/2016 FSX.pdf:application/pdf},
}

@article{srikanth_requirements_2016,
	title = {Requirements {Based} {Test} {Prioritization} {Using} {Risk} {Factors}},
	volume = {69},
	issn = {0950-5849},
	url = {https://biblioproxy.cnr.it:2481/10.1016/j.infsof.2015.09.002},
	doi = {10.1016/j.infsof.2015.09.002},
	abstract = {Context Software testing is an expensive and time-consuming process. Software engineering teams are often forced to terminate their testing efforts due to budgetary and time constraints, which inevitably lead to long term issues with quality and customer satisfaction. Test case prioritization (TCP) has shown to improve test effectiveness. Objective The results of our prior work on requirements-based test prioritization showed improved rate of fault detection on industrial projects; the customer priority (CP) and the fault proneness (FP) were the biggest contributing factors to test effectiveness. The objective of this paper is to further investigate these two factors and apply prioritization based on these factors in a different domain: an enterprise level cloud application. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of these two factors with two risk-based prioritization approaches that extract risks from the system requirements categories. Method Our approach involved analyzing and assigning values to each requirement based on two important factors, CP and FP, so that the test cases for high-value requirements are prioritized earlier for execution. We also proposed two requirements-based TCP approaches that use risk information of the system. Results Our results indicate that the use of CP and FP can improve the effectiveness of TCP. The results also show that the risk-based prioritization can be effective in improving the TCP. Conclusion We performed an experiment on an enterprise cloud application to measure the fault detection rate of different test suites that are prioritized based on CP, FP, and risks. The results depict that all approaches outperform the random prioritization approach, which is prevalent in the industry. Furthermore, the proposed approaches can easily be used in the industry to address the schedule and budget constraints at the testing phase.},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Srikanth, Hema and Hettiarachchi, Charitha and Do, Hyunsook},
	month = jan,
	year = {2016},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {\_tablet, Software testing, Test prioritization, System testing, Cloud application, SaaS},
	pages = {71--83},
	file = {2016 Requirements Based Test Prioritization Using Risk Factors.pdf:/Users/renangreca/Zotero/storage/W89GU68J/2016 Requirements Based Test Prioritization Using Risk Factors.pdf:application/pdf},
}

@inproceedings{machalica_predictive_2018,
	title = {Predictive {Test} {Selection}},
	url = {http://arxiv.org/abs/1810.05286},
	abstract = {Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentiallyimpacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95\% of individual test failures and over 99.9\% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test ﬂakiness.},
	language = {en},
	urldate = {2019-08-07},
	booktitle = {{arXiv}:1810.05286 [cs]},
	author = {Machalica, Mateusz and Samylkin, Alex and Porth, Meredith and Chandra, Satish},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.05286},
	keywords = {\_tablet, industry, continuous integration, Continuous Integration, Computer Science - Software Engineering, facebook, Machine Learning, flaky tests, test selection, machine learning, Flaky Tests, Test Selection},
	file = {2018 Predictive Test Selection.pdf:/Users/renangreca/Zotero/storage/EA2PMRZ4/2018 Predictive Test Selection.pdf:application/pdf},
}

@inproceedings{strandberg_experience_2016,
	title = {Experience {Report}: {Automated} {System} {Level} {Regression} {Test} {Prioritization} {Using} {Multiple} {Factors}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013287871&doi=10.1109%2fISSRE.2016.23&partnerID=40&md5=c1f959194819cf028e7cfd905528a71d},
	doi = {10.1109/ISSRE.2016.23},
	abstract = {We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced. © 2016 IEEE.},
	booktitle = {Proceedings - {International} {Symposium} on {Software} {Reliability} {Engineering}, {ISSRE}},
	author = {Strandberg, P.E. and Sundmark, D. and Afzal, W. and Ostrand, T.J. and Weyuker, E.J.},
	year = {2016},
	keywords = {\_tablet},
	pages = {12--23},
	file = {2016 Experience Report.pdf:/Users/renangreca/Zotero/storage/79VY864T/2016 Experience Report.pdf:application/pdf},
}

@article{lubke_selecting_2020,
	title = {Selecting and {Prioritizing} {Regression} {Test} {Suites} by {Production} {Usage} {Risk} in {Time}-{Constrained} {Environments}},
	doi = {10.1007/978-3-030-35510-4},
	abstract = {Regression Testing is an important quality assurance activity for combating unwanted side-effects, which might have been introduced in a new software release. Selecting and prioritizing regression test cases is a challenge in practice – especially in a world of ever increasing complex- ity, distribution, and size of the software solutions. Current approaches try to minimize the number of regression test cases by analyzing the change and the coverage of the tests with regards to this change. Our approach utilizes usage frequencies from the previous, productive soft- ware version in order to select or prioritize test cases by calculating the Regression Risk of a change. This takes into account that not all features of a software are used the same. We successfully validate our approach in a case study of an industry project which develops a complex process integration platform.},
	author = {Lübke, Daniel},
	year = {2020},
	note = {ISBN: 9783030355098},
	keywords = {\_tablet, test coverage, test selection, regression test, regression risk coverage, risk, software test, test priorization},
	pages = {69--83},
	file = {2020 Selecting and Prioritizing Regression Test Suites by Production Usage Risk in.pdf:/Users/renangreca/Zotero/storage/YXG89XFB/2020 Selecting and Prioritizing Regression Test Suites by Production Usage Risk in.pdf:application/pdf},
}

@inproceedings{celik_regression_2018,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2018},
	title = {Regression {Test} {Selection} for {TizenRT}},
	isbn = {978-1-4503-5573-5},
	url = {https://biblioproxy.cnr.it:2481/10.1145/3236024.3275527},
	doi = {10.1145/3236024.3275527},
	abstract = {Regression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C\#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform. We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95\% and end-to-end execution time to 7.04\% when tests are executed in the emulator, and to 5.74\% and 26.82\% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible.},
	booktitle = {Proceedings of the 2018 26th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Celik, Ahmet and Lee, Young Chul and Gligoric, Milos},
	month = oct,
	year = {2018},
	keywords = {\_tablet, Regression test selection, static dependency analysis, TizenRT},
	pages = {845--850},
	file = {2018 Regression Test Selection for TizenRT.pdf:/Users/renangreca/Zotero/storage/RS8VGMS3/2018 Regression Test Selection for TizenRT.pdf:application/pdf},
}

@inproceedings{buchgeher_improving_2016,
	title = {Improving testing in an enterprise {SOA} with an architecture-based approach},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983346282&doi=10.1109%2fWICSA.2016.24&partnerID=40&md5=23ec2b7191b79530b34e7aafeff10e65},
	doi = {10.1109/WICSA.2016.24},
	abstract = {High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis. © 2016 IEEE.},
	booktitle = {Proceedings - 2016 13th {Working} {IEEE}/{IFIP} {Conference} on {Software} {Architecture}, {WICSA} 2016},
	author = {Buchgeher, G. and Klammer, C. and Heider, W. and Schuetz, M. and Huber, H.},
	year = {2016},
	keywords = {\_tablet, Testing, Test Case Prioritization, Architecture Knowledge, Architecture-based Testing, SOA, Test Management},
	pages = {231--240},
	file = {2016 Improving testing in an enterprise SOA with an architecture-based approach.pdf:/Users/renangreca/Zotero/storage/38ASE24B/2016 Improving testing in an enterprise SOA with an architecture-based approach.pdf:application/pdf},
}

@incollection{zarges_artificial_2021,
	address = {Cham},
	title = {An {Artificial} {Immune} {System} for {Black} {Box} {Test} {Case} {Selection}},
	volume = {12692},
	isbn = {978-3-030-72903-5 978-3-030-72904-2},
	url = {http://link.springer.com/10.1007/978-3-030-72904-2_11},
	abstract = {Testing is a crucial part of the development of a new product. For software validation a transformation from manual to automated tests can be observed which enables companies to implement large numbers of test cases. However, during testing situations may occur where it is not feasible to run all tests due to time constraints. Hence a set of critical test cases must be compiled which usually fulﬁlls several criteria. Within this work we focus on criteria that are feasible for black box testing such as system tests. We adapt an existing artiﬁcial immune system for our use case and evaluate our method in a series of experiments using industrial datasets. We compare our approach with several other test selection methods where our algorithm shows superior performance.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Evolutionary {Computation} in {Combinatorial} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Rosenbauer, Lukas and Stein, Anthony and Hähner, Jörg},
	editor = {Zarges, Christine and Verel, Sébastien},
	year = {2021},
	doi = {10.1007/978-3-030-72904-2_11},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {\_tablet},
	pages = {169--184},
	file = {2021 An Artificial Immune System for Black Box Test Case Selection.pdf:/Users/renangreca/Zotero/storage/7SYKXYN5/2021 An Artificial Immune System for Black Box Test Case Selection.pdf:application/pdf},
}

@inproceedings{zhang_comparing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Comparing and combining analysis-based and learning-based regression test selection},
	isbn = {978-1-4503-9286-0},
	url = {https://dl.acm.org/doi/10.1145/3524481.3527230},
	doi = {10.1145/3524481.3527230},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} {International} {Conference} on {Automation} of {Software} {Test}},
	publisher = {ACM},
	author = {Zhang, Jiyang and Liu, Yu and Gligoric, Milos and Legunsen, Owolabi and Shi, August},
	month = may,
	year = {2022},
	keywords = {\_tablet},
	pages = {17--28},
	file = {2022 Comparing and combining analysis-based and learning-based regression test.pdf:/Users/renangreca/Zotero/storage/7UI9SPEI/2022 Comparing and combining analysis-based and learning-based regression test.pdf:application/pdf},
}

@inproceedings{abdelkarim_tcp-net_2022,
	address = {Valencia, Spain},
	title = {{TCP}-{Net}: {Test} {Case} {Prioritization} using {End}-to-{End} {Deep} {Neural} {Networks}},
	isbn = {978-1-66549-628-5},
	shorttitle = {{TCP}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9787970/},
	doi = {10.1109/ICSTW55395.2022.00034},
	abstract = {Regression testing is facing a bottleneck due to the growing number of test cases and the wide adoption of continuous integration (CI) in software projects, which increases the frequency of running software builds, making it challenging to run all the regression test cases. Machine learning (ML) techniques can be used to save time and hardware resources without compromising quality. In this work, we introduce a novel end-to-end, self-conﬁgurable, and incremental learning deep neural network (DNN) tool for test case prioritization (TCPNet). TCP-Net is fed with source code-related features, test case metadata, test case coverage information, and test case failure history, to learn a high dimensional correlation between source ﬁles and test cases. We experimentally show that TCP-Net can be efﬁciently used for test case prioritization by evaluating it on three different real-life industrial software packages.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	publisher = {IEEE},
	author = {Abdelkarim, Mohamed and ElAdawi, Reem},
	month = apr,
	year = {2022},
	keywords = {\_tablet},
	pages = {122--129},
	file = {2022 TCP-Net.pdf:/Users/renangreca/Zotero/storage/IKVL2GJH/2022 TCP-Net.pdf:application/pdf},
}

@inproceedings{xu_requirement-based_2021,
	address = {Madrid, Spain},
	title = {A {Requirement}-based {Regression} {Test} {Selection} {Technique} in {Behavior}-{Driven} {Development}},
	isbn = {978-1-66542-463-9},
	url = {https://ieeexplore.ieee.org/document/9529903/},
	doi = {10.1109/COMPSAC51774.2021.00182},
	abstract = {Regression testing is an essential software maintenance activity before the release of a new version implementing a bug ﬁx or a new feature. A regression test selection (RTS) technique chooses a subset of existing test cases to ensure that the system will not be adversely affected by the latest modiﬁcations. With the rise of DevOps, behavior-driven development (BDD) is growing in popularity as it is in close alignment with agile practices, for example, continuous integration. Hence, it is necessary to propose a novel and effective RTS technique for BDD speciﬁcally to accelerate the development process while ensuring software quality. Since most existing techniques for RTS are code-based and thus subject to some limitations, we present a requirement-based technique which uses the requirements in BDD to select test cases in both high-level (acceptance testing) and low-level (unit testing). Our technique ﬁrstly illustrates the new requirement with a scenario, and subsequently computes the semantic similarity of the new scenario and all existing scenarios with the vector space model. According to the results, the modiﬁcation-traversing regression test cases can be selected in a semi-automated way. We also conduct an experimental study to evaluate our technique in terms of inclusiveness, precision, efﬁciency and generality. The study shows that our technique is applicable for BDD and effective in practice.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	publisher = {IEEE},
	author = {Xu, Jincheng and Du, Qingfeng and Li, Xiaojun},
	month = jul,
	year = {2021},
	keywords = {\_tablet},
	pages = {1303--1308},
	file = {2021 A Requirement-based Regression Test Selection Technique in Behavior-Driven.pdf:/Users/renangreca/Zotero/storage/HG35DVIS/2021 A Requirement-based Regression Test Selection Technique in Behavior-Driven.pdf:application/pdf},
}

@inproceedings{sharif_deeporder_2021,
	address = {Luxembourg},
	title = {{DeepOrder}: {Deep} {Learning} for {Test} {Case} {Prioritization} in {Continuous} {Integration} {Testing}},
	isbn = {978-1-66542-882-8},
	shorttitle = {{DeepOrder}},
	url = {https://ieeexplore.ieee.org/document/9609187/},
	doi = {10.1109/ICSME52107.2021.00053},
	abstract = {Continuous integration testing is an important step in the modern software engineering life cycle. Test prioritization is a method that can improve the efficiency of continuous integration testing by selecting test cases that can detect faults in the early stage of each cycle. As continuous integration testing produces voluminous test execution data, test history is a commonly used artifact in test prioritization. However, existing test prioritization techniques for continuous integration either cannot handle large test history or are optimized for using a limited number of historical test cycles. We show that such a limitation can decrease fault detection effectiveness of prioritized test suites.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Sharif, Aizaz and Marijan, Dusica and Liaaen, Marius},
	month = sep,
	year = {2021},
	keywords = {\_tablet},
	pages = {525--534},
	file = {2021 DeepOrder.pdf:/Users/renangreca/Zotero/storage/JPDBSGRU/2021 DeepOrder.pdf:application/pdf},
}

@inproceedings{chen_multi-objective_2021,
	title = {Multi-{Objective} {Regression} {Test} {Selection}},
	url = {https://easychair.org/publications/paper/pgdP},
	doi = {10.29007/7z5n},
	abstract = {Regression testing is challenging, yet essential, for maintaining evolving complex software. Eﬃcient regression testing that minimizes the regression testing time and maximizes the detection of the regression faults is in great demand for fast-paced software development. Many research studies have been proposed for selecting regression tests under a time constraint. This paper presents a new approach that ﬁrst evaluates the fault detectability of each regression test based on the extent to which the test is impacted by the changes. Then, two optimization algorithms are proposed to optimize a multi-objective function that takes fault detectability and execution time of the test as inputs to select an optimal subset of the regression tests that can detect maximal regression faults under a given time constraint. The validity and eﬃcacy of the approach were evaluated using two empirical studies on industrial systems. The promising results suggest that the proposed approach has great potential to ensure the quality of the fast-paced evolving systems.},
	language = {en},
	urldate = {2022-09-14},
	year = {2021},
	author = {Chen, Yizhen and Chen, Mei-Hwa},
	keywords = {\_tablet},
	pages = {105--92},
	file = {Multi-Objective Regression Test Selection.pdf:/Users/renangreca/Zotero/storage/XPWQIWUH/Multi-Objective Regression Test Selection.pdf:application/pdf},
}

@article{lima_multi-armed_2022,
	title = {A {Multi}-{Armed} {Bandit} {Approach} for {Test} {Case} {Prioritization} in {Continuous} {Integration} {Environments}},
	volume = {48},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9086053/},
	doi = {10.1109/TSE.2020.2992428},
	abstract = {Continuous Integration (CI) environments have been increasingly adopted in the industry to allow frequent integration of software changes, making software evolution faster and cost-effective. In such environments, Test Case Prioritization (TCP) techniques play an important role to reduce regression testing costs, establishing a test case execution order that usually maximizes early fault detection. Existing works on TCP in CI environments (TCPCI) present some limitations. Few pieces of work consider CI particularities, such as the test case volatility, that is, they do not consider the dynamic environment of the software life-cycle in which new test cases can be added or removed (discontinued), characteristic related to the Exploration versus Exploitation (EvE) dilemma. To solve such a dilemma an approach needs to balance: i) the diversity of test suite; and ii) the quantity of new test cases and test cases that are errorprone or that comprise high fault-detection capabilities. To deal with this, most approaches use, besides the failure-history, other measures that rely on code instrumentation or require additional information, such as testing coverage. However, to maintain the information updated can be difﬁcult and time-consuming, not scalable due to the test budget of CI environments. In this context, and to properly deal with the TCPCI problem, this work presents an approach based on Multi-Armed Bandit (MAB) called COLEMAN (Combinatorial VOlatiLE Multi-Armed BANdit). The TCPCI problem falls into the category of volatile and combinatorial MAB, because multiple arms (test cases) need to be selected, and they are added or removed over the cycles. We conducted an evaluation considering three time budgets and eleven systems. The results show the applicability of our approach and that COLEMAN outperforms the most similar approach from literature in terms of early fault detection and performance.},
	language = {en},
	number = {2},
	urldate = {2022-09-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Lima, Jackson A. Prado and Vergilio, Silvia Regina},
	month = feb,
	year = {2022},
	keywords = {\_tablet},
	pages = {453--465},
	file = {2022 A Multi-Armed Bandit Approach for Test Case Prioritization in Continuous.pdf:/Users/renangreca/Zotero/storage/HQLV7WNV/2022 A Multi-Armed Bandit Approach for Test Case Prioritization in Continuous.pdf:application/pdf},
}

@article{zhou_parallel_2022,
	title = {Parallel {Test} {Prioritization}},
	volume = {31},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3471906},
	doi = {10.1145/3471906},
	abstract = {Although regression testing is important to guarantee the software quality in software evolution, it suffers from the widely known cost problem. To address this problem, existing researchers made dedicated efforts on test prioritization, which optimizes the execution order of tests to detect faults earlier; while practitioners in industry leveraged more computing resources to save the time cost of regression testing. By combining these two orthogonal solutions, in this article, we define the problem of parallel test prioritization, which is to conduct test prioritization in the scenario of parallel test execution to reduce the cost of regression testing.
            Different from traditional sequential test prioritization, parallel test prioritization aims at generating a set of test sequences, each of which is allocated in an individual computing resource and executed in parallel. In particular, we propose eight parallel test prioritization techniques by adapting the existing four sequential test prioritization techniques, by including and excluding testing time in prioritization.
            
              To investigate the performance of the eight parallel test prioritization techniques, we conducted an extensive study on 54 open-source projects and a case study on 16 commercial projects from
              Baidu
              , a famous search service provider with 600M monthly active users. According to the two studies, parallel test prioritization does improve the efficiency of regression testing, and cost-aware additional parallel test prioritization technique significantly outperforms the other techniques, indicating that this technique is a good choice for practical parallel testing. Besides, we also investigated the influence of two external factors, the number of computing resources and time allowed for parallel testing, and find that more computing resources indeed improve the performance of parallel test prioritization. In addition, we investigated the influence of two more factors, test granularity and coverage criterion, and find that parallel test prioritization can still accelerate regression testing in parallel scenario. Moreover, we investigated the benefit of parallel test prioritization on the regression testing process of continuous integration, considering both the cumulative acceleration performance and the overhead of prioritization techniques, and the results demonstrate the superiority of parallel test prioritization.},
	language = {en},
	number = {1},
	urldate = {2022-09-14},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Zhou, Jianyi and Chen, Junjie and Hao, Dan},
	month = jan,
	year = {2022},
	keywords = {\_tablet},
	pages = {1--50},
	file = {2022 Parallel Test Prioritization.pdf:/Users/renangreca/Zotero/storage/GRMI9W7X/2022 Parallel Test Prioritization.pdf:application/pdf},
}

@article{yaraghi_scalable_2022,
	title = {Scalable and {Accurate} {Test} {Case} {Prioritization} in {Continuous} {Integration} {Contexts}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9801672/},
	doi = {10.1109/TSE.2022.3184842},
	abstract = {Continuous Integration (CI) requires efﬁcient regression testing to ensure software quality without signiﬁcantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we ﬁrst deﬁne, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we deﬁne a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the deﬁned features for 25 open-source software systems with enough failed builds and whose regression testing takes at least ﬁve minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.},
	language = {en},
	urldate = {2022-09-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Yaraghi, Ahmadreza Saboor and Bagherzadeh, Mojtaba and Kahani, Nafiseh and Briand, Lionel},
	year = {2022},
	keywords = {\_tablet},
	pages = {1--24},
	file = {2022 Scalable and Accurate Test Case Prioritization in Continuous Integration.pdf:/Users/renangreca/Zotero/storage/B74YVF57/2022 Scalable and Accurate Test Case Prioritization in Continuous Integration.pdf:application/pdf},
}

@inproceedings{oqvist_extraction-based_2016,
	address = {Lugano Switzerland},
	title = {Extraction-{Based} {Regression} {Test} {Selection}},
	isbn = {978-1-4503-4135-6},
	url = {https://dl.acm.org/doi/10.1145/2972206.2972224},
	doi = {10.1145/2972206.2972224},
	abstract = {Frequent regression testing is a core activity in agile software development, but large test suites can lead to long test running times, hampering agility. By safe RTS (Regression Test Selection) techniques, a subset of the tests can be identiﬁed that cover all tests that can change result since the last run. To pay oﬀ in practice, the RTS overhead must be low. Most existing RTS techniques are based on dynamic coverage analysis, making the overhead related to the tests run. We present Extraction-Based RTS, a new safe RTS technique which uses a fast static analysis with very low overhead, related to the size of the modiﬁcation rather than to the tests run. The method is suitable for program-driven testing, commonly used in agile development, where each test is a piece of code that uses parts of the system under test. We have implemented the method for Java, and benchmarked it on a number of open source projects, showing that it pays oﬀ substantially in practice.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Öqvist, Jesper and Hedin, Görel and Magnusson, Boris},
	month = aug,
	year = {2016},
	keywords = {\_tablet},
	pages = {1--10},
	file = {2016 Extraction-Based Regression Test Selection.pdf:/Users/renangreca/Zotero/storage/TTW76N98/2016 Extraction-Based Regression Test Selection.pdf:application/pdf},
}

@inproceedings{bertolino_learning--rank_2020,
	address = {Seoul South Korea},
	title = {Learning-to-rank vs ranking-to-learn: strategies for regression testing in continuous integration},
	isbn = {978-1-4503-7121-6},
	shorttitle = {Learning-to-rank vs ranking-to-learn},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380369},
	doi = {10.1145/3377811.3380369},
	abstract = {In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-torank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bertolino, Antonia and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
	month = jun,
	year = {2020},
	keywords = {\_tablet},
	pages = {1--12},
	file = {2020 Learning-to-rank vs ranking-to-learn.pdf:/Users/renangreca/Zotero/storage/DRTDBZ88/2020 Learning-to-rank vs ranking-to-learn.pdf:application/pdf},
}

@inproceedings{pan_dynamic_2020,
	address = {Singapore Singapore},
	title = {Dynamic {Time} {Window} based {Reward} for {Reinforcement} {Learning} in {Continuous} {Integration} {Testing}},
	isbn = {978-1-4503-8819-1},
	url = {https://dl.acm.org/doi/10.1145/3457913.3457930},
	doi = {10.1145/3457913.3457930},
	abstract = {Continuous Integration (CI) testing is an expensive, time-consuming, and resource-intensive process. Test case prioritization (TCP) can effectively reduce the workload of regression testing in the CI environment, where Reinforcement Learning (RL) is adopted to prioritize test cases, since the TCP in CI testing can be formulated as a sequential decision-making problem, which can be solved by RL effectively. A useful reward function is a crucial component in the construction of the CI system and a critical factor in determining RL’s learning performance in CI testing. This paper focused on the validity of the execution history information of the test cases on the TCP performance in the existing CI testing optimization methods based on RL, and a Dynamic Time Window based reward function are proposed by using partial information dynamically for fast feedback and cost reduction. Experimental studies are carried out on six industrial datasets. The experimental results showed that using dynamic time window based reward function can significantly improve the learning efficiency of RL and the fault detection ability when comparing with the reward function based on fixed time window.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {12th {Asia}-{Pacific} {Symposium} on {Internetware}},
	publisher = {ACM},
	author = {Pan, Chaoyue and Yang, Yang and Li, Zheng and Guo, Junxia},
	month = nov,
	year = {2020},
	keywords = {\_tablet},
	pages = {189--198},
	file = {2020 Dynamic Time Window based Reward for Reinforcement Learning in Continuous.pdf:/Users/renangreca/Zotero/storage/X9WY4XFW/2020 Dynamic Time Window based Reward for Reinforcement Learning in Continuous.pdf:application/pdf},
}

@article{magalhaes_hsp_2020,
	title = {{HSP}: {A} hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study},
	volume = {159},
	issn = {01641212},
	shorttitle = {{HSP}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121219302043},
	doi = {10.1016/j.jss.2019.110430},
	language = {en},
	urldate = {2022-09-14},
	journal = {Journal of Systems and Software},
	author = {Magalhães, Claudio and Andrade, João and Perrusi, Lucas and Mota, Alexandre and Barros, Flávia and Maia, Eliot},
	month = jan,
	year = {2020},
	keywords = {\_tablet},
	pages = {110430},
	file = {2020 HSP.pdf:/Users/renangreca/Zotero/storage/4EMAMGI5/2020 HSP.pdf:application/pdf},
}

@inproceedings{elsner_empirically_2021,
	address = {Virtual Denmark},
	title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
	isbn = {978-1-4503-8459-9},
	url = {https://dl.acm.org/doi/10.1145/3460319.3464834},
	doi = {10.1145/3460319.3464834},
	abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	month = jul,
	year = {2021},
	keywords = {\_tablet},
	pages = {491--504},
	file = {2021 Empirically evaluating readily available information for regression test.pdf:/Users/renangreca/Zotero/storage/4W37X8IV/2021 Empirically evaluating readily available information for regression test.pdf:application/pdf},
}

@article{bagherzadeh_reinforcement_2022,
	title = {Reinforcement {Learning} for {Test} {Case} {Prioritization}},
	volume = {48},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9394799/},
	doi = {10.1109/TSE.2021.3070549},
	abstract = {Continuous Integration (CI) signiﬁcantly reduces integration problems, speeds up development time, and shortens release time. However, it also introduces new challenges for quality assurance activities, including regression testing, which is the focus of this work. Though various approaches for test case prioritization have shown to be very promising in the context of regression testing, speciﬁc techniques must be designed to deal with the dynamic nature and timing constraints of CI. Recently, Reinforcement Learning (RL) has shown great potential in various challenging scenarios that require continuous adaptation, such as game playing, real-time ads bidding, and recommender systems. Inspired by this line of work and building on initial efforts in supporting test case prioritization with RL techniques, we perform here a comprehensive investigation of RL-based test case prioritization in a CI context. To this end, taking test case prioritization as a ranking problem, we model the sequential interactions between the CI environment and a test case prioritization agent as an RL problem, using three alternative ranking models. We then rely on carefully selected and tailored state-ofthe-art RL techniques to automatically and continuously learn a test case prioritization strategy, whose objective is to be as close as possible to the optimal one. Our extensive experimental analysis shows that the best RL solutions provide a signiﬁcant accuracy improvement over previous RL-based work, with prioritization strategies getting close to being optimal, thus paving the way for using RL to prioritize test cases in a CI context.},
	language = {en},
	number = {8},
	urldate = {2022-09-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Bagherzadeh, Mojtaba and Kahani, Nafiseh and Briand, Lionel},
	month = aug,
	year = {2022},
	keywords = {\_tablet},
	pages = {2836--2856},
	file = {2022 Reinforcement Learning for Test Case Prioritization.pdf:/Users/renangreca/Zotero/storage/FXBJFB66/2022 Reinforcement Learning for Test Case Prioritization.pdf:application/pdf},
}

@article{li_aga_2021,
	title = {{AGA}: {An} {Accelerated} {Greedy} {Additional} {Algorithm} for {Test} {Case} {Prioritization}},
	issn = {0098-5589, 1939-3520, 2326-3881},
	shorttitle = {{AGA}},
	url = {https://ieeexplore.ieee.org/document/9662236/},
	doi = {10.1109/TSE.2021.3137929},
	abstract = {In recent years, many test case prioritization (TCP) techniques have been proposed to speed up the process of fault detection. However, little work has taken the efficiency problem of these techniques into account. In this paper, we target the Greedy Additional (GA) algorithm, which has been widely recognized to be effective but less efficient, and try to improve its efficiency while preserving effectiveness. In our Accelerated GA (AGA) algorithm, we use some extra data structures to reduce redundant data accesses in the GA algorithm and thus the time complexity is reduced from O(m2n) to O(kmn) when n {\textgreater} m, where m is the number of test cases, n is the number of program elements, and k is the iteration number. Moreover, we observe the impact of iteration numbers on prioritization efficiency on our dataset and propose to use a specific iteration number in the AGA algorithm to further improve the efficiency. We conducted experiments on 55 open-source subjects. In particular, we implemented each TCP algorithm with two kinds of widely-used input formats, adjacency matrix and adjacency list. Since a TCP algorithm with adjacency matrix is less efficient than the algorithm with adjacency list, the result analysis is mainly conducted based on TCP algorithms with adjacency list. The results show that AGA achieves 5.95X speedup ratio over GA on average, while it achieves the same average effectiveness as GA in terms of Average Percentage of Fault Detected (APFD). Moreover, we conducted an industrial case study on 22 subjects, collected from Baidu, and find that the average speedup ratio of AGA over GA is 44.27X, which indicates the practical usage of AGA in real-world scenarios.},
	language = {en},
	urldate = {2022-09-14},
	journal = {IEEE Transactions on Software Engineering},
	author = {Li, Feng and Zhou, Jianyi and Li, Yinzhu and Hao, Dan and Zhang, Lu},
	year = {2021},
	keywords = {\_tablet},
	pages = {1--1},
	file = {2021 AGA.pdf:/Users/renangreca/Zotero/storage/F6IS8G9G/2021 AGA.pdf:application/pdf},
}

@inproceedings{chen_context-aware_2021,
	address = {Taipei, Taiwan},
	title = {Context-{Aware} {Regression} {Test} {Selection}},
	isbn = {978-1-66543-784-4},
	url = {https://ieeexplore.ieee.org/document/9711972/},
	doi = {10.1109/APSEC53868.2021.00050},
	abstract = {Most modern software systems are continuously evolving, with changes frequently taking place in the core components or the execution context. These changes can adversely introduce regression faults, causing previously working functions to fail. Regression testing is essential for maintaining the quality of evolving complex software, but it can be overly time-consuming when the size of the test suite is large, or the execution of the test cases takes a long time. There are extensive research studies on selective regression testing aiming at minimizing the size of the regression test suite while maximizing the detection of the regression faults. However, most of the existing techniques focus on the regression faults caused by the code changes, the impact of the context changes on the non-modiﬁed software has barely been explored.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {2021 28th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	publisher = {IEEE},
	author = {Chen, Yizhen and Chaudhari, Ninad and Chen, Mei-Hwa},
	month = dec,
	year = {2021},
	keywords = {\_tablet},
	pages = {431--440},
	file = {2021 Context-Aware Regression Test Selection.pdf:/Users/renangreca/Zotero/storage/L5MF32DM/2021 Context-Aware Regression Test Selection.pdf:application/pdf},
}

@inproceedings{mehta_data-driven_2021,
	address = {Athens Greece},
	title = {Data-driven test selection at scale},
	isbn = {978-1-4503-8562-6},
	url = {https://dl.acm.org/doi/10.1145/3468264.3473916},
	doi = {10.1145/3468264.3473916},
	abstract = {Large-scale services depend on Continuous Integration/Continuous Deployment (CI/CD) processes to maintain their agility and codequality. Change-based testing plays an important role in finding bugs, but testing after every change is prohibitively expensive at a scale where thousands of changes are committed every hour. Test selection models deal with this issue by running a subset of tests for every change.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Mehta, Sonu and Farmahinifarahani, Farima and Bhagwan, Ranjita and Guptha, Suraj and Jafari, Sina and Kumar, Rahul and Saini, Vaibhav and Santhiar, Anirudh},
	month = aug,
	year = {2021},
	keywords = {\_tablet},
	pages = {1225--1235},
	file = {2021 Data-driven test selection at scale.pdf:/Users/renangreca/Zotero/storage/8DZYRXET/2021 Data-driven test selection at scale.pdf:application/pdf},
}

@article{magalhaes_ui_2021,
	title = {{UI} {Test} case prioritization on an industrial setting: {A} search for the best criteria},
	volume = {29},
	issn = {0963-9314, 1573-1367},
	shorttitle = {{UI} {Test} case prioritization on an industrial setting},
	url = {https://link.springer.com/10.1007/s11219-021-09549-y},
	doi = {10.1007/s11219-021-09549-y},
	abstract = {This work was developed in an industrial setting towards UI regression testing, where we do not have access to source code and the majority of test cases are manually executed (and only part of the regression-based test cases can be executed due to limited resources). Test case prioritization (TCP) is indicated for such a scenario. But characteristic of many TCP techniques is that they rely on source code coverage information, whereas we just have access to test cases, change requests, and their features. Thus, our goal is to investigate which criteria is the most relevant for prioritization. Thus, according to the literature we create an optimization model based on historical data. This model is embedded in a constraint solver designed for optimization. Our optimization function is based on the APFD (Average of the Percentage of Faults Detected) metric, but other metrics can be used as well. We have found that our partner already uses an appropriate criterion to identify failures which is statistically equivalent to other criteria used in experiments using our optimization model.},
	language = {en},
	number = {2},
	urldate = {2022-09-14},
	journal = {Software Quality Journal},
	author = {Magalhães, Claudio and Mota, Alexandre and Momente, Luis},
	month = jun,
	year = {2021},
	keywords = {\_tablet},
	pages = {381--403},
	file = {2021 UI Test case prioritization on an industrial setting.pdf:/Users/renangreca/Zotero/storage/YJIMZH3A/2021 UI Test case prioritization on an industrial setting.pdf:application/pdf},
}

@article{omri_learning_2022,
	title = {Learning to {Rank} for {Test} {Case} {Prioritization}},
	abstract = {In Continuous Integration (CI) environments, the productivity of software engineers depends strongly on the ability to reduce the round-trip time between code commits and feedback on failed test cases. Test case prioritization is popularly used as an optimization mechanism for ranking tests by their likelihood in revealing failures. However, existing techniques are usually time and resource intensive making them not suitable to be applied within CI cycles. This paper formulates the test case prioritization problem as an online learn-to-rank model using reinforcement learning techniques. Our approach minimizes the testing overhead and continuously adapts to the changing environment as new code and new test cases are added in each CI cycle. We validated our approach on an industrial case study showing that over 95\% of the test failures are still reported back to the software engineers while only 40\% of the total available test cases are being executed.},
	language = {en},
	author = {Omri, Safa and Sinz, Carsten},
	year = {2022},
	keywords = {\_tablet},
	pages = {9},
	file = {2022 Learning to Rank for Test Case Prioritization.pdf:/Users/renangreca/Zotero/storage/YPM6JCEJ/2022 Learning to Rank for Test Case Prioritization.pdf:application/pdf},
}

@inproceedings{cingil_black-box_2022,
	address = {Gothenburg Sweden},
	title = {Black-box {Test} {Case} {Selection} by {Relating} {Code} {Changes} with {Previously} {Fixed} {Defects}},
	isbn = {978-1-4503-9613-4},
	url = {https://dl.acm.org/doi/10.1145/3530019.3530023},
	doi = {10.1145/3530019.3530023},
	abstract = {Software continuously changes to address new requirements and to fix defects. Regression testing is performed to ensure that the applied changes do not adversely affect existing functionality. The increasing number of test cases makes it infeasible to execute the whole regression test suite. Test case selection is adopted to select a subset of the test suite, which is associated with the changed parts of the software. These parts are assumed to be error-prone. We present and evaluate a test case selection approach in the context of black-box regression testing of embedded systems. In this context, it is challenging to relate test cases with a set of distinct source code elements to be able to select those test cases associated with the modified parts of the source code. We analyze previously fixed defects for this purpose. We relate test cases that detect these defects with the source files that are previously modified for fixing them. Then, we select test cases related with source code files that are modified in the subsequent revision. The strength of this relation is determined as the number of changes associated with fixed defects previously detected by the same test cases. We conduct a case study on 3 real projects from the consumer electronics domain. Results show that it is possible to detect from 65\% up to 85\% of the defects detected by the whole test suite by selecting from 30\% up to 70\% of the test cases.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {The {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering} 2022},
	publisher = {ACM},
	author = {Çıngıl, Tutku and Sözer, Hasan},
	month = jun,
	year = {2022},
	keywords = {\_tablet},
	pages = {30--39},
	file = {2022 Black-box Test Case Selection by Relating Code Changes with Previously Fixed.pdf:/Users/renangreca/Zotero/storage/JMNNLRY6/2022 Black-box Test Case Selection by Relating Code Changes with Previously Fixed.pdf:application/pdf},
}

@inproceedings{greca_comparing_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Comparing and combining file-based selection and similarity-based prioritization towards regression test orchestration},
	isbn = {978-1-4503-9286-0},
	url = {https://dl.acm.org/doi/10.1145/3524481.3527223},
	doi = {10.1145/3524481.3527223},
	abstract = {Test case selection (TCS) and test case prioritization (TCP) techniques can reduce time to detect the first test failure. Although these techniques have been extensively studied in combination and isolation, they have not been compared one against the other. In this paper, we perform an empirical study directly comparing TCS and TCP approaches, represented by the tools Ekstazi and FAST, respectively. Furthermore, we develop the first combination, named Fastazi, of file-based TCS and similarity-based TCP and evaluate its benefit and cost against each individual technique. We performed our experiments using 12 Java-based open-source projects. Our results show that, in the median case, the combined approach detects the first failure nearly two times faster than either Ekstazi alone (with random test ordering) or FAST alone (without TCS). Statistical analysis shows that the effectiveness of Fastazi is higher than that of Ekstazi, which in turn is higher than that of FAST. On the other hand, FAST adds the least overhead to testing time, while the difference between the additional time needed by Ekstazi and Fastazi is negligible. Fastazi can also improve failure detection in scenarios where the time available for testing is restricted.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} {International} {Conference} on {Automation} of {Software} {Test}},
	publisher = {ACM},
	author = {Greca, Renan and Miranda, Breno and Gligoric, Milos and Bertolino, Antonia},
	month = may,
	year = {2022},
	keywords = {\_tablet},
	pages = {115--125},
	file = {2022 Comparing and combining file-based selection and similarity-based.pdf:/Users/renangreca/Zotero/storage/Z5QL8CYY/2022 Comparing and combining file-based selection and similarity-based.pdf:application/pdf},
}

@inproceedings{spieker_reinforcement_2017,
	address = {Santa Barbara CA USA},
	title = {Reinforcement learning for automatic test case prioritization and selection in continuous integration},
	isbn = {978-1-4503-5076-1},
	url = {https://dl.acm.org/doi/10.1145/3092703.3092709},
	doi = {10.1145/3092703.3092709},
	abstract = {Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.},
	language = {en},
	urldate = {2022-09-14},
	booktitle = {Proceedings of the 26th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten},
	month = jul,
	year = {2017},
	keywords = {\_tablet},
	pages = {12--22},
	file = {2017 Reinforcement learning for automatic test case prioritization and selection in.pdf:/Users/renangreca/Zotero/storage/PPV8IMDQ/2017 Reinforcement learning for automatic test case prioritization and selection in.pdf:application/pdf},
}
